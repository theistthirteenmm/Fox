# ุฑุงูููุง ูุตุจ ุฑูุจุงู ๐ฆ

## ูพุดโูุงุฒูุง

### 1. Python 3.8+
```bash
# ุจุฑุฑุณ ูุณุฎู Python
python --version
```

### 2. Node.js 16+
```bash
# ุจุฑุฑุณ ูุณุฎู Node.js
node --version
npm --version
```

### 3. Ollama (ูุณุชู ููุด ูุตููุน)

#### Windows:
1. ุงุฒ [ollama.ai](https://ollama.ai) ุฏุงูููุฏ ฺฉูุฏ
2. ูุงู ูุตุจ ุฑุง ุงุฌุฑุง ฺฉูุฏ
3. ูุฏู ููุฑุฏ ูุงุฒ ุฑุง ุฏุงูููุฏ ฺฉูุฏ:
```cmd
ollama pull llama3.2:3b
```

#### Linux/Mac:
```bash
# ูุตุจ Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# ุฏุงูููุฏ ูุฏู
ollama pull llama3.2:3b
```

## ูุตุจ ุฎูุฏฺฉุงุฑ

### ุฑูุด ุณุงุฏู (ุชูุตู ุดุฏู):
```bash
python setup.py
```

ุงู ุงุณฺฉุฑูพุช ุชูุงู ูุฑุงุญู ูุตุจ ุฑุง ุฎูุฏฺฉุงุฑ ุงูุฌุงู ูโุฏูุฏ.

## ูุตุจ ุฏุณุช

### 1. Backend (Python)
```bash
# ุงุฌุงุฏ virtual environment
python -m venv venv

# ูุนุงูโุณุงุฒ (Windows)
venv\Scripts\activate

# ูุนุงูโุณุงุฒ (Linux/Mac)
source venv/bin/activate

# ูุตุจ dependencies
pip install -r requirements.txt
```

### 2. Frontend (React)
```bash
cd frontend
npm install
```

### 3. ุงุฌุงุฏ ุฏุงุฑฺฉุชูุฑโูุง ููุฑุฏ ูุงุฒ
```bash
mkdir -p data/memory
mkdir -p data/personality
mkdir -p data/learning
mkdir -p logs
```

## ุฑุงูโุงูุฏุงุฒ

### 1. ุดุฑูุน Ollama
```bash
ollama serve
```

### 2. ุดุฑูุน Backend
```bash
# ูุนุงูโุณุงุฒ virtual environment
source venv/bin/activate  # Linux/Mac
# ุง
venv\Scripts\activate     # Windows

# ุงุฌุฑุง ุณุฑูุฑ
python backend/main.py
```

### 3. ุดุฑูุน Frontend
```bash
cd frontend
npm start
```

### ุฑุงูโุงูุฏุงุฒ ุฎูุฏฺฉุงุฑ
ุจุนุฏ ุงุฒ ูุตุจุ ูโุชูุงูุฏ ุงุฒ ุงุณฺฉุฑูพุชโูุง ุขูุงุฏู ุงุณุชูุงุฏู ฺฉูุฏ:

**Windows:**
```cmd
start_robah.bat
```

**Linux/Mac:**
```bash
./start_robah.sh
```

## ุฏุณุชุฑุณ

- **ุฑุงุจุท ูุจ**: http://localhost:3000
- **API Backend**: http://localhost:8000
- **ูุณุชูุฏุงุช API**: http://localhost:8000/docs

## ุชูุธูุงุช

### ูุชุบุฑูุง ูุญุท
```bash
# ูุฏู AI (ุงุฎุชุงุฑ)
export ROBAH_MODEL="llama3.2:3b"

# ุขุฏุฑุณ Ollama (ุงุฎุชุงุฑ)
export OLLAMA_URL="http://localhost:11434"

# ูพูุฑุช ุณุฑูุฑ (ุงุฎุชุงุฑ)
export ROBAH_PORT=8000
```

### ูุงู ุชูุธูุงุช
ุชูุธูุงุช ุฏุฑ `config/settings.py` ูุงุจู ุชุบุฑ ุงุณุช.

## ุนุจโุงุจ

### ูุดฺฉูุงุช ุฑุงุฌ

#### 1. Ollama ุฏุฑ ุฏุณุชุฑุณ ูุณุช
```bash
# ุจุฑุฑุณ ูุถุนุช Ollama
curl http://localhost:11434/api/tags

# ุฑุงูโุงูุฏุงุฒ ูุฌุฏุฏ
ollama serve
```

#### 2. ูุฏู ุฏุงูููุฏ ูุดุฏู
```bash
# ุฏุงูููุฏ ูุฏู
ollama pull llama3.2:3b

# ุจุฑุฑุณ ูุฏูโูุง ูุตุจ ุดุฏู
ollama list
```

#### 3. ุฎุทุง ูพูุฑุช
```bash
# ุชุบุฑ ูพูุฑุช backend
export ROBAH_PORT=8001
python backend/main.py
```

#### 4. ูุดฺฉู ุญุงูุธู
```bash
# ูพุงฺฉ ฺฉุฑุฏู ุญุงูุธู (ุงุฎุชุงุฑ)
rm -rf data/memory/*
```

### ูุงฺฏโูุง
ูุงฺฏโูุง ุณุณุชู ุฏุฑ `logs/robah.log` ุฐุฎุฑู ูโุดููุฏ.

## ุจูโุฑูุฒุฑุณุงู

```bash
# ุจูโุฑูุฒุฑุณุงู dependencies
pip install -r requirements.txt --upgrade
cd frontend && npm update
```

## ูพุดุชุจุงู

ุฏุฑ ุตูุฑุช ูุดฺฉู:
1. ูุงฺฏโูุง ุฑุง ุจุฑุฑุณ ฺฉูุฏ
2. ูุฑุงุญู ูุตุจ ุฑุง ุฏูุจุงุฑู ุงูุฌุงู ุฏูุฏ
3. ูุทูุฆู ุดูุฏ ุชูุงู ูพุดโูุงุฒูุง ูุตุจ ุดุฏูโุงูุฏ

---

๐ฆ **ุฑูุจุงู ุขูุงุฏู ุงุณุช ุชุง ุจุง ุดูุง ุฑุดุฏ ฺฉูุฏ!**