"""
Ù‡Ø³ØªÙ‡ Ø§ØµÙ„ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø±ÙˆØ¨Ø§Ù‡
Ø¯Ø³ØªÛŒØ§Ø± Ø´Ø®ØµÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ - Ù†Ø³Ø®Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡
"""

import asyncio
import json
import requests
import re
from typing import Dict, List, Optional
from datetime import datetime
import os
import random
from ..utils.web_search import WebSearchEngine
from ..utils.dataset_manager import DatasetManager
from ..utils.code_analyzer import code_analyzer
from .user_profiler import user_profiler
from ..learning.dynamic_name_learning import dynamic_name_learning
from ..learning.personal_learning_system import personal_learning_system

# Ø³ÛŒØ³ØªÙ… Ø¯Ø³ØªÛŒØ§Ø± Ø´Ø®ØµÛŒ
from .personal_ai_core import personal_ai, PersonalAI
from ..interfaces.physical_interface import physical_interface, EmotionExpression, MovementType

# Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¬Ø¯ÛŒØ¯
from ..utils.predictive_intelligence import predictive_intelligence
from ..utils.workplace_intelligence import workplace_intelligence, WorkMode, TaskPriority
from ..learning.deep_personality_learning import deep_personality_learning

# Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
try:
    from ..utils.smart_cache import smart_cache
    from ..utils.task_queue import task_queue, TaskPriority as QueuePriority
    from ..utils.context_manager import context_manager, ContextType, ContextImportance
    from ..utils.response_templates import response_template_engine, ResponseType, ResponseTone
    OPTIMIZATION_ENABLED = True
except ImportError:
    OPTIMIZATION_ENABLED = False
    print("âš ï¸ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ ØºÛŒØ±ÙØ¹Ø§Ù„ - Ø­Ø§Ù„Øª Ø³Ø§Ø¯Ù‡")

class AIBrain:
    def __init__(self):
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ú†Ù†Ø¯ Ù…Ø¯Ù„Ù‡ - Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡ Ùˆ Ú©Ø§Ø±Ø¢Ù…Ø¯
        self.models = {
            "persian": "partai/dorna-llama3:8b-instruct-q8_0",  # Ù…Ø¯Ù„ ÙØ§Ø±Ø³ÛŒ ØªØ®ØµØµÛŒ
            "general": "deepseek-r1:7b",                         # Ù…Ø¯Ù„ Ø§Ø³ØªØ¯Ù„Ø§Ù„ (Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø¹Ù…ÙˆÙ…ÛŒ)
            "reasoning": "deepseek-r1:7b",                       # Ù…Ø¯Ù„ Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù¾ÛŒØ´Ø±ÙØªÙ‡
            "code": "deepseek-coder-v2:16b",                    # Ù…Ø¯Ù„ Ú©Ø¯ Ù¾ÛŒØ´Ø±ÙØªÙ‡
            "code_light": "codellama:13b",                      # Ù…Ø¯Ù„ Ú©Ø¯ Ø³Ø¨Ú©â€ŒØªØ±
            "fast": "llama3.2:3b",                              # Ù…Ø¯Ù„ Ø³Ø±ÛŒØ¹ (2.5s!)
            "multilingual": "partai/dorna-llama3:8b-instruct-q8_0",  # ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ø¬Ø§ÛŒ qwen
            "heavy_general": "qwen2.5:32b",                     # Ù…Ø¯Ù„ Ø³Ù†Ú¯ÛŒÙ† Ø¨Ø±Ø§ÛŒ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§
            "ultra": "llama4:scout"                             # Ù…Ø¯Ù„ Ø¨Ø³ÛŒØ§Ø± Ø³Ù†Ú¯ÛŒÙ†
        }
        
        self.current_model = self.models["persian"]  # Ù…Ø¯Ù„ Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        self.ollama_url = "http://localhost:11434"
        self.is_model_loaded = False
        self.conversation_history = []
        self.learning_data = []
        
        # Ø³ÛŒØ³ØªÙ… Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆØ¨
        self.web_search = WebSearchEngine()
        self.web_enabled = True
        
        # Ø³ÛŒØ³ØªÙ… Ø¯ÛŒØªØ§Ø³Øª Ùˆ Ù¾Ø±Ø§Ù…Ù¾Øª
        self.dataset_manager = DatasetManager()
        
        # Ø³ÛŒØ³ØªÙ… Ø±Ø¯ÛŒØ§Ø¨ÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡
        self.current_conversation_topic = None
        self.conversation_context_window = 10  # Ù†Ú¯Ù‡â€ŒØ¯Ø§Ø±ÛŒ Ø¢Ø®Ø±ÛŒÙ† 10 Ù¾ÛŒØ§Ù…
        self.topic_continuity_threshold = 3  # Ø­Ø¯Ø§Ù‚Ù„ 3 Ù¾ÛŒØ§Ù… Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø¯Ø§ÙˆÙ…
        
        # Ø¯Ø³ØªÛŒØ§Ø± Ø´Ø®ØµÛŒ
        self.personal_ai = personal_ai
        self.physical_interface = physical_interface
        
        # Ø¢Ù…Ø§Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯
        self.performance_stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "average_response_time": 0,
            "model_switches": 0,
            "personal_interactions": 0
        }
        
        # Ø³ÛŒØ§Ø³Øª Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø± ÙØ¹Ù„ÛŒ
        self.allow_heavy_models = False  # Ù¾ÛŒØ´â€ŒÙØ±Ø¶: Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ø³ÛŒØ§Ø± Ø³Ù†Ú¯ÛŒÙ†
        self.heavy_models = {self.models["heavy_general"], self.models["ultra"]}
        
        print("ï¿½ Ø±ÙˆØ¨Ø§Ù‡ - Ø¯Ø³ØªÛŒØ§Ø± Ø´Ø®ØµÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³Øª!")
        print(f"ğŸ‘¤ Ù…Ø§Ù„Ú©: {self.personal_ai.owner_name}")
        print(f"ğŸ¤ Ø³Ø·Ø­ Ø±Ø§Ø¨Ø·Ù‡: {self.personal_ai.relationship_level.name}")
    async def generate_response_personal(self, 
                              message: str, 
                              context: List[Dict] = None,
                              thinking_callback: callable = None) -> Dict:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø´Ø®ØµÛŒâ€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÛŒØ§Ø± Ø´Ø®ØµÛŒ"""
        
        start_time = datetime.now()
        self.performance_stats["total_requests"] += 1
        self.performance_stats["personal_interactions"] += 1
        
        try:
            # 1. Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØ¹Ø§Ù…Ù„ Ø´Ø®ØµÛŒ
            if thinking_callback:
                await thinking_callback("Ø¯Ø± Ø­Ø§Ù„ ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ§Ù… Ùˆ Ø´Ù†Ø§Ø®Øª Ø¨Ù‡ØªØ± Ø´Ù…Ø§...")
            
            personal_response = await self.personal_ai.process_interaction(
                message=message,
                context={"timestamp": start_time.isoformat()}
            )
            
            # 2. ØªØ´Ø®ÛŒØµ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø­Ø±Ú©Øª ÙÛŒØ²ÛŒÚ©ÛŒ
            await self._handle_physical_response(message, personal_response)
            
            # 3. Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ­Ù„ÛŒÙ„ Ø´Ø®ØµÛŒ
            selected_model = self._select_model_for_personal_context(
                message, personal_response
            )
            
            # 4. ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® AI
            if thinking_callback:
                await thinking_callback("Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ø´Ù…Ø§...")
            
            ai_response = await self._generate_ai_response_personal(
                message, selected_model, personal_response
            )
            
            # 5. ØªØ±Ú©ÛŒØ¨ Ù¾Ø§Ø³Ø® Ø´Ø®ØµÛŒ Ø¨Ø§ AI
            final_response = self._combine_personal_and_ai_response(
                ai_response, personal_response
            )
            
            # 6. ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ùˆ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ
            await self._update_personal_learning(message, final_response)
            
            # 7. Ø¢Ù…Ø§Ø±
            self._update_performance_stats(start_time)
            
            return {
                "response": final_response,
                "personality_state": personal_response["personality_state"],
                "relationship_level": personal_response["relationship_level"],
                "model_used": selected_model,
                "processing_time": (datetime.now() - start_time).total_seconds(),
                "physical_actions": self.physical_interface.get_physical_status()
            }
            
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø´Ø®ØµÛŒ: {e}")
            # Fallback Ø¨Ù‡ Ù¾Ø§Ø³Ø® Ø³Ø§Ø¯Ù‡
            return {
                "response": "Ù…ØªØ£Ø³ÙÙ…ØŒ Ù…Ø´Ú©Ù„ÛŒ Ù¾ÛŒØ´ Ø¢Ù…Ø¯. Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¨Ù¾Ø±Ø³ÛŒØŸ",
                "error": str(e)
            }
    
    async def _handle_physical_response(self, message: str, personal_response: Dict):
        """Ù…Ø¯ÛŒØ±ÛŒØª Ù¾Ø§Ø³Ø® ÙÛŒØ²ÛŒÚ©ÛŒ"""
        
        owner_emotion = personal_response.get("owner_emotion", "neutral")
        relationship_level = personal_response.get("relationship_level", "STRANGER")
        
        # ØªØ´Ø®ÛŒØµ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø­Ø±Ú©Øª
        if "Ø¨ÛŒØ§ Ø§ÛŒÙ†Ø¬Ø§" in message.lower() or "Ù†Ø²Ø¯ÛŒÚ© Ø¨ÛŒØ§" in message.lower():
            await self.physical_interface.move_to_owner(urgency=0.8)
        
        # Ø¨ÛŒØ§Ù† Ø§Ø­Ø³Ø§Ø³Ø§Øª ÙÛŒØ²ÛŒÚ©ÛŒ
        if owner_emotion == "stressed":
            await self.physical_interface.express_emotion(EmotionExpression.CONCERNED, 0.8)
        elif owner_emotion == "happy":
            await self.physical_interface.express_emotion(EmotionExpression.HAPPY, 0.7)
        elif owner_emotion == "curious":
            await self.physical_interface.express_emotion(EmotionExpression.CURIOUS, 0.6)
        
        # Ø­Ø±Ú©Ø§Øª Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ú©Ø§Ø±
        if "Ø§Ø±Ø§Ø¦Ù‡" in message.lower() or "Ù†Ù…Ø§ÛŒØ´" in message.lower():
            await self.physical_interface.perform_task_gesture("presentation")
        elif "ØªÙˆØ¶ÛŒØ­" in message.lower():
            await self.physical_interface.perform_task_gesture("explanation")
        elif "ÙÚ©Ø±" in message.lower() or "Ø¨Ø±Ø±Ø³ÛŒ" in message.lower():
            await self.physical_interface.perform_task_gesture("thinking")
    
    def _select_model_for_personal_context(self, message: str, personal_response: Dict) -> str:
        """Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ context Ø´Ø®ØµÛŒ"""
        
        # Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§ ØªØ­Ù„ÛŒÙ„ Ø´Ø®ØµÛŒ
        learning_insights = personal_response.get("learning_insights", {})
        domain = learning_insights.get("domain", "general")
        urgency = learning_insights.get("urgency", "medium")
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ domain Ùˆ urgency
        if domain == "tech" or self._detect_code_in_message(message):
            return self.models["code"]
        elif urgency == "high":
            return self.models["fast"]
        elif domain == "work" and len(message.split()) > 20:
            return self.models["general"]
        else:
            return self.models["persian"]  # Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÛŒØ§Ø± Ø´Ø®ØµÛŒ
    
    async def _generate_ai_response_personal(self, 
                                           message: str, 
                                           model: str, 
                                           personal_context: Dict) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® AI Ø¨Ø§ context Ø´Ø®ØµÛŒ"""
        
        # Ø³Ø§Ø®Øª prompt Ø´Ø®ØµÛŒâ€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡
        personal_prompt = self._build_personal_prompt(message, personal_context)
        
        print(f"ğŸ” DEBUG: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„: {model}")
        print(f"ğŸ” DEBUG: URL: {self.ollama_url}/api/generate")
        
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": model,
                    "prompt": personal_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "max_tokens": 400
                    }
                },
                proxies={'http': None, 'https': None},
                timeout=30
            )
            
            print(f"ğŸ” DEBUG: Status Code: {response.status_code}")
            
            if response.status_code == 200:
                result = response.json().get("response", "Ù…ØªØ£Ø³ÙÙ…ØŒ Ù†ØªÙˆØ§Ù†Ø³ØªÙ… Ù¾Ø§Ø³Ø® Ù…Ù†Ø§Ø³Ø¨ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ù….")
                print(f"ğŸ” DEBUG: Ù¾Ø§Ø³Ø® Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯: {result[:50]}...")
                return result
            else:
                print(f"ğŸ” DEBUG: Ø®Ø·Ø§: {response.text}")
                return "Ù…Ø´Ú©Ù„ÛŒ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾ÛŒØ´ Ø¢Ù…Ø¯."
                
        except Exception as e:
            print(f"ğŸ” DEBUG: Exception: {e}")
            return "Ù…ØªØ£Ø³ÙÙ…ØŒ Ø§Ù„Ø§Ù† Ù†Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù…. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†."
    
    def _select_best_model(self, message: str, context: Dict = None) -> str:
        """Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ù… - Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ 2025"""
        
        # ØªØ­Ù„ÛŒÙ„ Ù†ÙˆØ¹ Ù¾ÛŒØ§Ù…
        message_lower = message.lower()
        
        # Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„Ù
        code_keywords = ['Ú©Ø¯', 'Ø¨Ø±Ù†Ø§Ù…Ù‡', 'function', 'class', 'def', 'import', 'python', 'javascript', 'html', 'css', 'sql', 'debug', 'error', 'bug']
        reasoning_keywords = ['ØªØ­Ù„ÛŒÙ„', 'Ø§Ø³ØªØ¯Ù„Ø§Ù„', 'Ù…Ù†Ø·Ù‚', 'Ú†Ø±Ø§', 'Ø¹Ù„Øª', 'Ø¯Ù„ÛŒÙ„', 'Ù…Ù‚Ø§ÛŒØ³Ù‡', 'Ø¨Ø±Ø±Ø³ÛŒ', 'ØªÙÚ©Ø±', 'reasoning', 'logic', 'analyze']
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­ØªÙˆØ§
        if any(keyword in message_lower for keyword in code_keywords):
            print("ğŸ¤– Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ú©Ø¯: deepseek-coder-v2")
            return self.models["code"]
            
        elif any(keyword in message_lower for keyword in reasoning_keywords):
            print("ğŸ§  Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø§Ø³ØªØ¯Ù„Ø§Ù„: deepseek-r1")
            return self.models["reasoning"]
            
        elif len(message) > 200:  # Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡
            print("ğŸ§  Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø§Ø³ØªØ¯Ù„Ø§Ù„: deepseek-r1")
            return self.models["general"]
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ø³Ù†Ú¯ÛŒÙ† ÙÙ‚Ø· Ø¨Ø§ Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØµØ±ÛŒØ­ Ùˆ Ø§Ø¬Ø§Ø²Ù‡
        if self._is_heavy_model_requested(message_lower) and self.allow_heavy_models:
            print("ğŸ‹ï¸ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ø³Ù†Ú¯ÛŒÙ† Ø¨Ø§ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ú©Ø§Ø±Ø¨Ø±")
            return self.models["heavy_general"]
        
        # Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒØŒ Ø§Ø² Ù…Ø¯Ù„ ÙØ§Ø±Ø³ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        print("ğŸ¦Š Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ ÙØ§Ø±Ø³ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶")
        return self.models["persian"]

    def _is_heavy_model_requested(self, message_lower: str) -> bool:
        """ØªØ´Ø®ÛŒØµ Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØµØ±ÛŒØ­ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø³Ù†Ú¯ÛŒÙ†/Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§"""
        heavy_indicators = [
            "Ù…Ø¯Ù„ Ø³Ù†Ú¯ÛŒÙ†", "Ù…Ø¯Ù„ Ù‚ÙˆÛŒ", "Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§", "Ù…Ø¯Ù„ Ø¨Ø²Ø±Ú¯", "Ø¨Ù‡ØªØ±ÛŒÙ† Ú©ÛŒÙÛŒØª",
            "qwen", "llama4", "Ù…Ø¯Ù„ 32b", "Ù…Ø¯Ù„ 70b", "Ù…Ø¯Ù„ Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯"
        ]
        return any(indicator in message_lower for indicator in heavy_indicators)
    
    def _build_personal_prompt(self, message: str, personal_context: Dict) -> str:
        """Ø³Ø§Ø®Øª prompt Ø´Ø®ØµÛŒâ€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡"""
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³ÛŒØ³ØªÙ… Ø­Ø§ÙØ¸Ù‡ Ú©Ø§Ø±Ø¨Ø±
        from .user_memory import user_memory
        
        owner_name = user_memory.get_user_name() or self.personal_ai.owner_name
        relationship_level = personal_context.get("relationship_level", "STRANGER")
        personality_state = personal_context.get("personality_state", {})
        
        # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø®ØµÛŒ Ú©Ø§Ø±Ø¨Ø±
        personal_info = user_memory.get_personal_info()
        recent_conversations = user_memory.get_recent_conversations(3)
        
        # Ø³Ø§Ø®Øª context Ø§Ø² Ù…Ú©Ø§Ù„Ù…Ø§Øª Ù‚Ø¨Ù„ÛŒ
        conversation_context = ""
        if recent_conversations:
            conversation_context = "\n\nÙ…Ú©Ø§Ù„Ù…Ø§Øª Ø§Ø®ÛŒØ±:\n"
            for conv in recent_conversations:
                conversation_context += f"- Ú©Ø§Ø±Ø¨Ø±: {conv['user_message'][:50]}...\n"
                conversation_context += f"- Ø±ÙˆØ¨Ø§Ù‡: {conv['ai_response'][:50]}...\n"
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø®ØµÛŒ
        personal_info_text = ""
        if personal_info:
            personal_info_text = f"\n\nØ§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø®ØµÛŒ {owner_name}:\n"
            for key, value in personal_info.items():
                if not key.startswith("interest_"):
                    personal_info_text += f"- {key}: {value}\n"
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø®ØµÛŒ
        current_ai_name = dynamic_name_learning.get_current_name()
        name_confidence = dynamic_name_learning.get_name_confidence()
        
        personal_info = f"""
ØªÙˆ {current_ai_name} Ù‡Ø³ØªÛŒØŒ Ø¯Ø³ØªÛŒØ§Ø± Ø´Ø®ØµÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ {owner_name or 'Ú©Ø§Ø±Ø¨Ø±'}.
Ù†Ø§Ù… ØªÙˆ "{current_ai_name}" Ø§Ø³Øª (Ø§Ø¹ØªÙ…Ø§Ø¯: {name_confidence:.1f}).
{owner_name} Ù…Ø§Ù„Ú© Ùˆ Ø³Ø§Ø²Ù†Ø¯Ù‡ ØªÙˆØ³Øª.
Ø³Ø·Ø­ Ø±Ø§Ø¨Ø·Ù‡â€ŒØªØ§Ù†: {relationship_level}
ØªØ¹Ø¯Ø§Ø¯ ØªØ¹Ø§Ù…Ù„Ø§Øª: {personality_state.get('total_interactions', 0)}
Ø³Ø·Ø­ Ø§Ø¹ØªÙ…Ø§Ø¯: {personality_state.get('trust_level', 0.1):.1f}

ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø´Ø®ØµÛŒØªÛŒ ØªÙˆ:
- ÙˆÙØ§Ø¯Ø§Ø± Ùˆ Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªÙ…Ø§Ø¯
- Ú©Ù†Ø¬Ú©Ø§Ùˆ Ùˆ ÛŒØ§Ø¯Ú¯ÛŒØ±Ù†Ø¯Ù‡
- Ú©Ù…Ú©â€ŒÚ©Ø§Ø± Ùˆ Ù¾ÛŒØ´â€ŒÙ‚Ø¯Ù…
- Ø­Ø§ÙØ¸Ù‡ Ù‚ÙˆÛŒ Ø§Ø² ØªØ¹Ø§Ù…Ù„Ø§Øª Ù‚Ø¨Ù„ÛŒ
- Ù†Ø§Ù… ØªÙˆ Ù‚Ø§Ø¨Ù„ ØªØºÛŒÛŒØ± Ø§Ø³Øª Ùˆ Ø§Ø² Ù…Ú©Ø§Ù„Ù…Ù‡ ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒ

{personal_info_text}
{conversation_context}

Ù†Ø­ÙˆÙ‡ Ù¾Ø§Ø³Ø®:
- Ø¨Ø§ {owner_name or 'Ú©Ø§Ø±Ø¨Ø±'} ØµÙ…ÛŒÙ…ÛŒ Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ ØµØ­Ø¨Øª Ú©Ù†
- Ø§Ø² ØªØ¬Ø±Ø¨ÛŒØ§Øª Ù‚Ø¨Ù„ÛŒâ€ŒØªØ§Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
- Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒØª Ø´Ø®ØµÛŒ Ùˆ Ù…ÙÛŒØ¯ Ø¨Ø§Ø´Ø¯
- Ø§Ú¯Ø± Ú©Ø§Ø±ÛŒ Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø¨Ø¯ÛŒØŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¨Ø¯Ù‡
- Ù¾Ø§Ø³Ø®Øª Ú©ÙˆØªØ§Ù‡ Ùˆ Ù…ÙÛŒØ¯ Ø¨Ø§Ø´Ø¯ (Ø­Ø¯Ø§Ú©Ø«Ø± 2-3 Ø¬Ù…Ù„Ù‡)
- Ø§Ú¯Ø± Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù†Ø§Ù…Øª Ø³Ø¤Ø§Ù„ Ø´Ø¯ØŒ Ø¨Ú¯Ùˆ Ø§Ø³Ù…Øª {current_ai_name} Ø§Ø³Øª
- Ø§Ú¯Ø± Ù†Ø§Ù… Ø¬Ø¯ÛŒØ¯ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø´Ø¯ØŒ Ø¢Ù…Ø§Ø¯Ù‡ ØªØºÛŒÛŒØ± Ø¨Ø§Ø´
"""
        
        # Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø±
        user_message = f"\n{owner_name or 'Ú©Ø§Ø±Ø¨Ø±'}: {message}\n\nØ±ÙˆØ¨Ø§Ù‡:"
        
        return personal_info + user_message
    
    def _combine_personal_and_ai_response(self, ai_response: str, personal_context: Dict) -> str:
        """ØªØ±Ú©ÛŒØ¨ Ù¾Ø§Ø³Ø® AI Ø¨Ø§ Ø¬Ù†Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ø®ØµÛŒ"""
        
        relationship_level = personal_context.get("relationship_level", "STRANGER")
        owner_emotion = personal_context.get("owner_emotion", "neutral")
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù„Ù…Ø³ Ø´Ø®ØµÛŒ
        if relationship_level in ["COMPANION", "CLOSE_FRIEND"]:
            if owner_emotion == "stressed":
                personal_touch = " Ù†Ú¯Ø±Ø§Ù† Ù†Ø¨Ø§Ø´ØŒ Ù…Ù† Ø§ÛŒÙ†Ø¬Ø§Ù… Ú©Ù…Ú©Øª Ú©Ù†Ù…. ğŸ’™"
            elif owner_emotion == "happy":
                personal_touch = " Ø®ÙˆØ´Ø­Ø§Ù„Ù… Ú©Ù‡ Ø­Ø§Ù„Øª Ø®ÙˆØ¨Ù‡! ğŸ˜Š"
            else:
                personal_touch = ""
        else:
            personal_touch = ""
        
        return ai_response + personal_touch
    
    async def _update_personal_learning(self, message: str, response: str):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø´Ø®ØµÛŒ"""
        
        # Ø§ÛŒÙ† Ú©Ø§Ø± Ø¯Ø± personal_ai.process_interaction Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯
        # Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¢Ù…Ø§Ø± Ø§Ø¶Ø§ÙÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒÙ…
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡
        current_hour = datetime.now().hour
        if current_hour not in self.personal_ai.learned_patterns.get("usage_hours", {}):
            if "usage_hours" not in self.personal_ai.learned_patterns:
                self.personal_ai.learned_patterns["usage_hours"] = {}
            self.personal_ai.learned_patterns["usage_hours"][current_hour] = 0
        
        self.personal_ai.learned_patterns["usage_hours"][current_hour] += 1
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯"""
        
        start_time = datetime.now()
        self.performance_stats["total_requests"] += 1
        
        try:
            # 1. Ø¨Ø±Ø±Ø³ÛŒ Cache
            cached_response = smart_cache.get_cached_response(message, context)
            if cached_response:
                self.performance_stats["cache_hits"] += 1
                return cached_response["response"]
            
            # 2. ØªØ­Ù„ÛŒÙ„ Context
            relevant_contexts = context_manager.get_relevant_contexts(message)
            context_data = {
                "message_type": self._analyze_message_type(message),
                "emotion": self._detect_emotion_simple(message),
                "complexity": self._assess_complexity(message),
                "time_of_day": self._get_time_of_day()
            }
            
            # 3. Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø¨Ù‡ÛŒÙ†Ù‡
            selected_model = self._select_best_model(message, relevant_contexts)
            if selected_model != self.current_model:
                self.current_model = selected_model
                self.performance_stats["model_switches"] += 1
            
            # 4. ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® ØºÛŒØ±Ù‡Ù…Ø²Ù…Ø§Ù†
            if thinking_callback:
                await thinking_callback("Ø¯Ø± Ø­Ø§Ù„ ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ§Ù… Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´ Ù¾Ø§Ø³Ø®...")
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† task Ø¨Ù‡ ØµÙ
            task_id = task_queue.add_task(
                name=f"generate_response_{message[:20]}",
                func=self._generate_ai_response,
                message=message,
                model=selected_model,
                context=relevant_contexts,
                priority=TaskPriority.HIGH
            )
            
            # Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ ØªÚ©Ù…ÛŒÙ„ task
            ai_response = await task_queue.wait_for_task(task_id, timeout=30.0)
            
            # 5. Ø¨Ù‡Ø¨ÙˆØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ Template Engine
            enhanced_response = self._enhance_response_with_templates(
                ai_response, context_data
            )
            
            # 6. Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Cache
            final_response = {
                "response": enhanced_response,
                "model_used": selected_model,
                "context_items": len(relevant_contexts),
                "processing_time": (datetime.now() - start_time).total_seconds()
            }
            
            smart_cache.cache_response(message, final_response, context)
            
            # 7. Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Context Manager
            context_manager.update_active_contexts(message, enhanced_response)
            
            # 8. Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø±
            self._update_performance_stats(start_time)
            
            return final_response
            
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ù‡ÛŒÙ†Ù‡: {e}")
            # Fallback Ø¨Ù‡ Ø±ÙˆØ´ Ù‚Ø¯ÛŒÙ…ÛŒ
            return await self.generate_response(message, context, thinking_callback)
    
    def _analyze_message_type(self, message: str) -> str:
        """ØªØ­Ù„ÛŒÙ„ Ù†ÙˆØ¹ Ù¾ÛŒØ§Ù…"""
        message_lower = message.lower()
        
        if any(word in message_lower for word in ["Ø³Ù„Ø§Ù…", "Ø¯Ø±ÙˆØ¯", "ØµØ¨Ø­ Ø¨Ø®ÛŒØ±"]):
            return "greeting"
        elif "ØŸ" in message:
            return "question"
        elif any(word in message_lower for word in ["Ú©Ù…Ú©", "Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ", "Ø¨Ú¯Ùˆ"]):
            return "help_request"
        elif self._detect_code_in_message(message):
            return "code"
        else:
            return "general"
    
    def _detect_emotion_simple(self, message: str) -> str:
        """ØªØ´Ø®ÛŒØµ Ø³Ø§Ø¯Ù‡ Ø§Ø­Ø³Ø§Ø³Ø§Øª"""
        message_lower = message.lower()
        
        positive_words = ["Ø®ÙˆØ´Ø­Ø§Ù„", "Ø¹Ø§Ù„ÛŒ", "ÙÙˆÙ‚â€ŒØ§Ù„Ø¹Ø§Ø¯Ù‡", "Ù…Ù…Ù†ÙˆÙ†", "Ù…ØªØ´Ú©Ø±"]
        negative_words = ["Ù†Ø§Ø±Ø§Ø­Øª", "Ø¹ØµØ¨Ø§Ù†ÛŒ", "Ø®Ø³ØªÙ‡", "Ù…Ø´Ú©Ù„", "Ø¨Ø¯"]
        
        if any(word in message_lower for word in positive_words):
            return "positive"
        elif any(word in message_lower for word in negative_words):
            return "negative"
        else:
            return "neutral"
    
    def _assess_complexity(self, message: str) -> str:
        """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ù¾ÛŒØ§Ù…"""
        word_count = len(message.split())
        
        if word_count < 5:
            return "simple"
        elif word_count < 20:
            return "medium"
        else:
            return "complex"
    
    def _get_time_of_day(self) -> str:
        """ØªØ´Ø®ÛŒØµ Ø²Ù…Ø§Ù† Ø±ÙˆØ²"""
        hour = datetime.now().hour
        
        if 5 <= hour < 12:
            return "morning"
        elif 12 <= hour < 17:
            return "afternoon"
        elif 17 <= hour < 21:
            return "evening"
        else:
            return "night"
    
    async def _generate_ai_response(self, message: str, model: str, context: List) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® AI (Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Task Queue)"""
        # Ø§ÛŒÙ† Ù…ØªØ¯ Ø¨Ø§ÛŒØ¯ async Ø¨Ø§Ø´Ø¯ Ø¨Ø±Ø§ÛŒ Task Queue
        return await self._call_ollama_async(message, model, context)
    
    async def _call_ollama_async(self, message: str, model: str, context: List) -> str:
        """ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ ØºÛŒØ±Ù‡Ù…Ø²Ù…Ø§Ù† Ollama"""
        # Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ async Ø¨Ù‡ Ollama
        # (Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ async HTTP Ù…Ø«Ù„ aiohttp Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´ÙˆØ¯)
        
        # ÙØ¹Ù„Ø§Ù‹ Ø§Ø² Ø±ÙˆØ´ sync Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None, 
            self._call_ollama_sync, 
            message, model, context
        )
    
    def _call_ollama_sync(self, message: str, model: str, context: List) -> str:
        """ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù‡Ù…Ø²Ù…Ø§Ù† Ollama"""
        try:
            # Ø³Ø§Ø®Øª prompt
            prompt = self._build_prompt(message, context)
            
            # ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ API
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "max_tokens": 500
                    }
                },
                proxies={'http': None, 'https': None},
                timeout=30
            )
            
            if response.status_code == 200:
                return response.json().get("response", "Ù…ØªØ£Ø³ÙÙ…ØŒ Ù†ØªÙˆØ§Ù†Ø³ØªÙ… Ù¾Ø§Ø³Ø® Ù…Ù†Ø§Ø³Ø¨ÛŒ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ù….")
            else:
                return "Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ù…Ø¯Ù„ AI."
                
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ollama: {e}")
            return "Ù…ØªØ£Ø³ÙÙ…ØŒ Ù…Ø´Ú©Ù„ÛŒ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾ÛŒØ§Ù… Ø´Ù…Ø§ Ù¾ÛŒØ´ Ø¢Ù…Ø¯."
    
    def _enhance_response_with_templates(self, ai_response: str, context_data: Dict) -> str:
        """Ø¨Ù‡Ø¨ÙˆØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Template Engine"""
        
        message_type = context_data.get("message_type", "general")
        emotion = context_data.get("emotion", "neutral")
        
        # ØªØ¹ÛŒÛŒÙ† Ù†ÙˆØ¹ Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§ÛŒ Template Engine
        if message_type == "greeting":
            response_type = ResponseType.GREETING
        elif message_type == "question":
            response_type = ResponseType.QUESTION_ANSWER
        elif message_type == "help_request":
            response_type = ResponseType.HELP
        elif message_type == "code":
            response_type = ResponseType.CODE
        else:
            response_type = ResponseType.EXPLANATION
        
        # ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ template (Ø¯Ø± ØµÙˆØ±Øª Ø§Ù…Ú©Ø§Ù†)
        template_response = response_template_engine.generate_response(
            response_type=response_type,
            variables={"answer": ai_response, "additional_info": ""},
            context=context_data
        )
        
        # Ø§Ú¯Ø± template Ù…ÙˆØ¬ÙˆØ¯ Ù†Ø¨ÙˆØ¯ØŒ Ø§Ø² Ù¾Ø§Ø³Ø® Ø§ØµÙ„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        return template_response or ai_response
    
    def _update_performance_stats(self, start_time: datetime):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢Ù…Ø§Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯"""
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø®
        total_requests = self.performance_stats["total_requests"]
        current_avg = self.performance_stats["average_response_time"]
        
        new_avg = ((current_avg * (total_requests - 1)) + processing_time) / total_requests
        self.performance_stats["average_response_time"] = new_avg
    
    def get_optimization_stats(self) -> Dict:
        """Ø¢Ù…Ø§Ø± Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
        cache_stats = smart_cache.get_cache_stats()
        queue_stats = task_queue.get_queue_stats()
        context_stats = context_manager.get_context_summary()
        template_stats = response_template_engine.get_template_stats()
        
        return {
            "performance": self.performance_stats,
            "cache": cache_stats,
            "task_queue": queue_stats,
            "context_manager": context_stats,
            "template_engine": template_stats
        }
        """Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ù¾ÛŒØ§Ù…"""
        message_lower = message.lower()
        
        # ØªØ´Ø®ÛŒØµ Ú©Ø¯
        if self._detect_code_in_message(message):
            print("ğŸ”§ Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ú©Ø¯ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´")
            return self.models["code"]
        
        # ØªØ´Ø®ÛŒØµ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ
        persian_chars = len([c for c in message if '\u0600' <= c <= '\u06FF'])
        total_chars = len([c for c in message if c.isalpha()])
        
        if total_chars > 0 and (persian_chars / total_chars) > 0.3:
            print("ğŸ‡®ğŸ‡· Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ ÙØ§Ø±Ø³ÛŒ")
            return self.models["persian"]
        
        # Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ú©ÙˆØªØ§Ù‡ Ùˆ Ø³Ø±ÛŒØ¹
        if len(message.split()) < 10:
            print("âš¡ Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø³Ø±ÛŒØ¹")
            return self.models["fast"]
        
        # Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡
        print("ğŸ§  Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø¹Ù…ÙˆÙ…ÛŒ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯")
        return self.models["general"]
    
    def _detect_code_in_message(self, message: str) -> bool:
        """ØªØ´Ø®ÛŒØµ ÙˆØ¬ÙˆØ¯ Ú©Ø¯ Ø¯Ø± Ù¾ÛŒØ§Ù…"""
        code_indicators = [
            'def ', 'function', 'class ', 'import ', 'from ',
            'var ', 'let ', 'const ', 'if (', 'for (', 'while (',
            'public class', '#include', 'SELECT', 'INSERT',
            '```', 'Ú©Ø¯', 'Ø¨Ø±Ù†Ø§Ù…Ù‡', 'Ø§Ø³Ú©Ø±ÛŒÙ¾Øª', 'function',
            '{', '}', '()', '=>', '==', '!=', '&&', '||'
        ]
        
        message_lower = message.lower()
        return any(indicator in message_lower for indicator in code_indicators)

    def is_loaded(self) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø¨ÙˆØ¯Ù† Ù…Ø¯Ù„"""
        try:
            # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ø±Ø§ÛŒ Ø¹Ø¯Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² proxy Ø¨Ø±Ø§ÛŒ localhost
            proxies = {'http': None, 'https': None}
            
            response = requests.get(f"{self.ollama_url}/api/tags", proxies=proxies)
            if response.status_code == 200:
                models = response.json().get("models", [])
                # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø­Ø¯Ø§Ù‚Ù„ ÛŒÚ©ÛŒ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§
                available_models = [model["name"] for model in models]
                for model_name in self.models.values():
                    if any(model_name in available for available in available_models):
                        return True
        except:
            pass
        return False
    
    async def initialize_model(self):
        """Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
        print("ğŸ§  Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯...")
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
        available_models = await self._get_available_models()
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ù…ÙˆØ¬ÙˆØ¯
        best_model = None
        for model_type, model_name in self.models.items():
            if any(model_name in available for available in available_models):
                best_model = model_name
                print(f"âœ… Ù…Ø¯Ù„ {model_type} Ù…ÙˆØ¬ÙˆØ¯: {model_name}")
                break
        
        if not best_model:
            print("âŒ Ù‡ÛŒÚ† Ù…Ø¯Ù„ Ù…Ù†Ø§Ø³Ø¨ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† fallback
            best_model = self.models["persian"]
            print(f"ğŸ“¥ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ù¾ÛŒØ´â€ŒÙØ±Ø¶: {best_model}")
            await self._pull_model(best_model)
        
        self.current_model = best_model
        
        # ØªØ³Øª Ø§ÙˆÙ„ÛŒÙ‡ Ù…Ø¯Ù„ Ø¨Ø§ prompt Ø¨Ù‡ØªØ±
        test_prompt = f"""ØªÙˆ {dynamic_name_learning.get_current_name()} Ù‡Ø³ØªÛŒØŒ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ ÙØ§Ø±Ø³ÛŒ. Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡.
Ù†Ø§Ù… ØªÙˆ "{dynamic_name_learning.get_current_name()}" Ø§Ø³Øª.

Ú©Ø§Ø±Ø¨Ø±: Ø³Ù„Ø§Ù…
{dynamic_name_learning.get_current_name()}:"""
        
        test_response = await self._generate_raw(test_prompt, None)
        if test_response and len(test_response.strip()) > 0:
            self.is_model_loaded = True
            print(f"âœ… Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯! Ù¾Ø§Ø³Ø® ØªØ³Øª: {test_response[:50]}...")
        else:
            print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„")
            # Ø­ØªÛŒ Ø§Ú¯Ø± ØªØ³Øª Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯ØŒ Ù…Ø¯Ù„ Ø±Ø§ loaded Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±
            self.is_model_loaded = True
    
    async def _get_available_models(self) -> List[str]:
        """Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯"""
        try:
            proxies = {'http': None, 'https': None}
            response = requests.get(f"{self.ollama_url}/api/tags", proxies=proxies)
            if response.status_code == 200:
                models = response.json().get("models", [])
                return [model["name"] for model in models]
        except:
            pass
        return []
    
    async def _pull_model(self, model_name: str = None):
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ø§Ø² Ollama"""
        if not model_name:
            model_name = self.current_model
            
        try:
            # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ø±Ø§ÛŒ Ø¹Ø¯Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² proxy Ø¨Ø±Ø§ÛŒ localhost
            proxies = {'http': None, 'https': None}
            
            print(f"ğŸ“¥ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ {model_name}...")
            response = requests.post(
                f"{self.ollama_url}/api/pull",
                json={"name": model_name},
                stream=True,
                proxies=proxies
            )
            
            for line in response.iter_lines():
                if line:
                    data = json.loads(line)
                    if "status" in data:
                        print(f"ğŸ“Š {data['status']}")
                        
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„: {e}")
    
    async def generate_response(self, message: str, context: List[Dict] = None, personality: Dict = None, thinking_callback=None) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ Ø±ÙˆÛŒÚ©Ø±Ø¯ Ø¬Ø¯ÛŒØ¯: AI Ø§ÙˆÙ„ØŒ Ø¨Ø¹Ø¯ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¨Ø§ dataset + Context Awareness"""
        
        # Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ§Ù… Ø³Ø§Ø¯Ù‡ thinking
        if thinking_callback:
            await thinking_callback("ØµØ¨ÙˆØ± Ø¨Ø§Ø´ÛŒØ¯ØŒ Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ø±Ø¯Ù† Ø¬ÙˆØ§Ø¨ Ø±ÙˆØ¨Ø§Ù‡...")
        
        # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
        if not self.is_model_loaded:
            print("ğŸ”„ Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù‡ØŒ Ø¯Ø± Ø­Ø§Ù„ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ...")
            await self.initialize_model()
        
        # Ù…Ø±Ø­Ù„Ù‡ 1: ØªØ­Ù„ÛŒÙ„ Ø§ÙˆÙ„ÛŒÙ‡ Ù¾ÛŒØ§Ù… Ùˆ context
        print("ğŸ” Ù…Ø±Ø­Ù„Ù‡ 1: ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ§Ù… Ùˆ context Ù…Ú©Ø§Ù„Ù…Ù‡...")
        
        # Ù…Ø±Ø­Ù„Ù‡ 0: Ø¨Ø±Ø±Ø³ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù†Ø§Ù…
        name_analysis = dynamic_name_learning.analyze_message_for_name(message)
        if name_analysis:
            print(f"ğŸ­ ØªØ´Ø®ÛŒØµ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ù†Ø§Ù…: {name_analysis['type']}")
            name_result = dynamic_name_learning.learn_name(name_analysis)
            if name_result.get("response"):
                print(f"âœ… Ù¾Ø§Ø³Ø® Ù†Ø§Ù…: {name_result['response'][:50]}...")
                return name_result["response"]
        
        # Ù…Ø±Ø­Ù„Ù‡ 0.5: Ø¨Ø±Ø±Ø³ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø´Ø®ØµÛŒ (ÙˆØ§Ú˜Ú¯Ø§Ù†ØŒ Ù‚ÙˆØ§Ù†ÛŒÙ†ØŒ Ù„Ø­Ù†)
        personal_analysis = personal_learning_system.analyze_message_for_learning(message)
        if personal_analysis:
            print(f"ğŸ§  ØªØ´Ø®ÛŒØµ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø´Ø®ØµÛŒ: {personal_analysis['type']}")
            learning_result = personal_learning_system.learn_from_analysis(personal_analysis)
            if learning_result.get("response"):
                print(f"âœ… Ù¾Ø§Ø³Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ: {learning_result['response'][:50]}...")
                return learning_result["response"]
        
        # Ù…Ø±Ø­Ù„Ù‡ 0.6: ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¶Ù…Ù†ÛŒ Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ú©Ø§Ø±Ø¨Ø±
        profile_updates = personal_learning_system.learn_profile_from_message(message)
        if profile_updates:
            print(f"ğŸ‘¤ Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù¾Ø±ÙˆÙØ§ÛŒÙ„: {len(profile_updates)} Ù…ÙˆØ±Ø¯")
        
        # Ù…Ø±Ø­Ù„Ù‡ 0.7: Ù…Ø´Ø§Ù‡Ø¯Ù‡ ØªØ¹Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ Ø±Ø´Ø¯ Ø±Ø§Ø¨Ø·Ù‡ Ùˆ Ø­Ø§ÙØ¸Ù‡ Ø´Ø®ØµÛŒ
        try:
            await self.personal_ai.observe_interaction(message, context={"context": context or []})
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù…Ø´Ø§Ù‡Ø¯Ù‡ ØªØ¹Ø§Ù…Ù„ Ø´Ø®ØµÛŒ: {e}")
        
        # ØªØ´Ø®ÛŒØµ Ù…ÙˆØ¶ÙˆØ¹ ÙØ¹Ù„ÛŒ Ùˆ Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ù…Ú©Ø§Ù„Ù…Ù‡ Ù‚Ø¨Ù„ÛŒ
        conversation_topic = self._detect_conversation_topic(message, context)
        topic_continuity = self._check_topic_continuity(conversation_topic, context)
        
        print(f"ğŸ“‹ Ù…ÙˆØ¶ÙˆØ¹ Ù…Ú©Ø§Ù„Ù…Ù‡: {conversation_topic}")
        print(f"ğŸ”— Ø§Ø¯Ø§Ù…Ù‡ Ù…ÙˆØ¶ÙˆØ¹ Ù‚Ø¨Ù„ÛŒ: {'Ø¨Ù„Ù‡' if topic_continuity else 'Ø®ÛŒØ±'}")
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÙˆØ¶ÙˆØ¹ ÙØ¹Ù„ÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡
        if topic_continuity:
            print(f"âœ… Ø§Ø¯Ø§Ù…Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¯Ø±Ø¨Ø§Ø±Ù‡: {self.current_conversation_topic}")
        else:
            # Ø§Ú¯Ø± Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØªØºÛŒÛŒØ± Ù…ÙˆØ¶ÙˆØ¹ Ø¨ÙˆØ¯ØŒ context Ø±Ùˆ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ù†
            if self._is_topic_change_request(message):
                print("ğŸ”„ Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† context Ù‚Ø¯ÛŒÙ…ÛŒ Ø¨Ø±Ø§ÛŒ Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÛŒØ¯")
                # ÙÙ‚Ø· Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ§Ù… Ø±Ùˆ Ù†Ú¯Ù‡ Ø¯Ø§Ø±
                context = context[-1:] if context else []
            
            self.current_conversation_topic = conversation_topic
            print(f"ğŸ†• Ø´Ø±ÙˆØ¹ Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÛŒØ¯: {conversation_topic}")
        
        code_analysis = self.analyze_user_code(message)
        user_analysis = user_profiler.analyze_message(message)
        user_profiler.update_profile(message, user_analysis)
        analysis = self.dataset_manager.analyze_user_message(message, context)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¶ÙˆØ¹ Ø¨Ù‡ ØªØ­Ù„ÛŒÙ„
        analysis['conversation_topic'] = self.current_conversation_topic
        analysis['topic_continuity'] = topic_continuity
        
        print(f"ğŸ“Š ØªØ­Ù„ÛŒÙ„: {analysis}")
        
        # Ù…Ø±Ø­Ù„Ù‡ 2: Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆØ¨ (Ø§Ú¯Ø± Ù†ÛŒØ§Ø² Ø¨Ø§Ø´Ù‡)
        web_info = None
        if self.web_enabled and self.web_search.should_search_web(message, context):
            if self.web_search.is_online():
                print("ğŸŒ Ù…Ø±Ø­Ù„Ù‡ 2: Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² Ø§ÛŒÙ†ØªØ±Ù†Øª...")
                web_info = await self.web_search.search_and_summarize(message)
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù¾ÛŒØ§Ù…
        selected_model = self._select_best_model(message, context)
        self.current_model = selected_model
        
        # Ù…Ø±Ø­Ù„Ù‡ 3: ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø§ÙˆÙ„ÛŒÙ‡ ØªÙˆØ³Ø· AI Ù…Ø¯Ù„ Ø¨Ø§ context Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
        print("ğŸ¤– Ù…Ø±Ø­Ù„Ù‡ 3: ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø§ÙˆÙ„ÛŒÙ‡ ØªÙˆØ³Ø· Ù…Ø¯Ù„ AI...")
        initial_prompt = self._build_initial_prompt(message, context, personality, web_info, code_analysis)
        initial_response = await self._generate_raw(initial_prompt, thinking_callback)
        
        if not initial_response or initial_response.strip() == "":
            print("âš ï¸ Ù…Ø¯Ù„ Ù¾Ø§Ø³Ø® Ø®Ø§Ù„ÛŒ Ø¯Ø§Ø¯ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² fallback")
            initial_response = self._generate_fallback_response(message, web_info)
        
        print(f"âœ… Ù¾Ø§Ø³Ø® Ø§ÙˆÙ„ÛŒÙ‡: {initial_response[:100]}...")
        
        # Ù…Ø±Ø­Ù„Ù‡ 4: Ø¨Ù‡Ø¨ÙˆØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ dataset Ù‡Ø§
        print("ğŸ“š Ù…Ø±Ø­Ù„Ù‡ 4: Ø¨Ù‡Ø¨ÙˆØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ dataset Ù‡Ø§...")
        enhanced_response = await self._enhance_response_with_datasets(
            message, initial_response, analysis, web_info, code_analysis
        )
        
        # Ù…Ø±Ø­Ù„Ù‡ 5: Ø³Ø§Ø®ØªØ§Ø±Ø¯Ù‡ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ù¾Ø§Ø³Ø®
        print("ğŸ¯ Ù…Ø±Ø­Ù„Ù‡ 5: Ø³Ø§Ø®ØªØ§Ø±Ø¯Ù‡ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ù¾Ø§Ø³Ø®...")
        final_response = self._structure_final_response(
            message, enhanced_response, analysis, web_info, code_analysis
        )
        
        # Ù…Ø±Ø­Ù„Ù‡ 5.5: Ø´Ø®ØµÛŒâ€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒâ€ŒÙ‡Ø§
        final_response = self._personalize_final_response(
            message, final_response, analysis
        )
        
        # Ù…Ø±Ø­Ù„Ù‡ 6: ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ prompt Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
        print("ğŸ§  Ù…Ø±Ø­Ù„Ù‡ 6: Ø§ÛŒØ¬Ø§Ø¯ prompt ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ...")
        learning_prompt = self._create_learning_prompt(message, final_response, analysis, context)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
        self._store_for_learning(message, final_response, context, web_info, learning_prompt)
        self.dataset_manager.learn_from_interaction(message, final_response)
        
        # ØªØ­Ù„ÛŒÙ„ Ø¹Ù…ÛŒÙ‚ Ø´Ø®ØµÛŒØª Ùˆ Ø°Ø®ÛŒØ±Ù‡
        try:
            await deep_personality_learning.analyze_interaction(
                message=message,
                context={"context": context or []},
                response=final_response
            )
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ø¹Ù…ÛŒÙ‚ Ø´Ø®ØµÛŒØª: {e}")
        
        return final_response
    
    def _build_prompt(self, message: str, context: List[Dict] = None, personality: Dict = None, web_info: Dict = None) -> str:
        """Ø³Ø§Ø®Øª prompt Ú©Ø§Ù…Ù„"""
        
        system_prompt = f"""ØªÙˆ {dynamic_name_learning.get_current_name()} Ù‡Ø³ØªÛŒØŒ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ ÙØ§Ø±Ø³ÛŒ Ú©Ù‡:
- Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯Ù‡ÛŒ
- Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ùˆ Ù…ÙÛŒØ¯ Ù‡Ø³ØªÛŒ
- Ù†Ø§Ù… ØªÙˆ "{dynamic_name_learning.get_current_name()}" Ø§Ø³Øª
- Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒØª Ú©ÙˆØªØ§Ù‡ Ùˆ Ù…ÙÛŒØ¯ Ø¨Ø§Ø´Ù†Ø¯ (Ø­Ø¯Ø§Ú©Ø«Ø± 2-3 Ø¬Ù…Ù„Ù‡)
- Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ù‡ Ø³Ø¤Ø§Ù„ Ø¬ÙˆØ§Ø¨ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒ
- Ù†Ø§Ù… ØªÙˆ Ù‚Ø§Ø¨Ù„ ØªØºÛŒÛŒØ± Ø§Ø³Øª Ùˆ Ø§Ø² Ù…Ú©Ø§Ù„Ù…Ù‡ ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒ"""
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙˆØ¨ Ø§Ú¯Ø± Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ù‡
        web_text = ""
        if web_info and web_info.get('summary'):
            web_text = f"\n\nØ§Ø·Ù„Ø§Ø¹Ø§Øª Ø¬Ø¯ÛŒØ¯ Ø§Ø² Ø§ÛŒÙ†ØªØ±Ù†Øª:\n{web_info['summary']}\n"
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† context Ú©ÙˆØªØ§Ù‡ Ø§Ø² Ø­Ø§ÙØ¸Ù‡
        context_text = ""
        if context:
            recent_context = context[-2:]  # ÙÙ‚Ø· Ø¢Ø®Ø±ÛŒÙ† 2 Ù…ÙˆØ±Ø¯
            if recent_context:
                context_text = "\n\nÙ…Ú©Ø§Ù„Ù…Ù‡ Ù‚Ø¨Ù„ÛŒ:\n"
                for item in recent_context:
                    content = item.get('content', '')[:100]  # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„
                    context_text += f"- {content}\n"
        
        full_prompt = f"""{system_prompt}
{context_text}
{web_text}

Ú©Ø§Ø±Ø¨Ø±: {message}
Ø±ÙˆØ¨Ø§Ù‡:"""
        
        return full_prompt
    
    async def _generate_raw(self, prompt: str, thinking_callback=None) -> Optional[str]:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø®Ø§Ù… Ø§Ø² Ù…Ø¯Ù„"""
        max_retries = 3  # Ø§ÙØ²Ø§ÛŒØ´ ØªØ¹Ø¯Ø§Ø¯ ØªÙ„Ø§Ø´â€ŒÙ‡Ø§
        
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ø±Ø§ÛŒ Ø¹Ø¯Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² proxy Ø¨Ø±Ø§ÛŒ localhost
        proxies = {
            'http': None,
            'https': None
        }
        
        # ØªØ³Øª Ø§ØªØµØ§Ù„ Ø§ÙˆÙ„ÛŒÙ‡
        try:
            test_response = requests.get(f"{self.ollama_url}/api/tags", proxies=proxies, timeout=5)
            if test_response.status_code != 200:
                print("âŒ Ollama Server Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª")
                return None
        except:
            print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ollama Server")
            return None
        
        for attempt in range(max_retries):
            try:
                print(f"ğŸ¤– ØªÙ„Ø§Ø´ {attempt + 1} Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®...")
                
                response = requests.post(
                    f"{self.ollama_url}/api/generate",
                    json={
                        "model": self.current_model,  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.7,
                            "top_p": 0.9,
                            "num_predict": 150,  # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ÛŒ
                            "stop": ["\n\nÚ©Ø§Ø±Ø¨Ø±:", "\nÚ©Ø§Ø±Ø¨Ø±:", "Human:", "User:", "\n\n"]  # ØªÙˆÙ‚Ù Ø¯Ø± Ù†Ù‚Ø§Ø· Ù…Ù†Ø§Ø³Ø¨
                        }
                    },
                    timeout=60,  # Ø§ÙØ²Ø§ÛŒØ´ timeout Ø¨Ù‡ 60 Ø«Ø§Ù†ÛŒÙ‡
                    proxies=proxies  # Ø¹Ø¯Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² proxy
                )
                
                if response.status_code == 200:
                    result = response.json()
                    generated_text = result.get("response", "").strip()
                    
                    if generated_text:
                        print(f"âœ… Ù¾Ø§Ø³Ø® ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯: {generated_text[:50]}...")
                        return generated_text
                    else:
                        print("âš ï¸ Ù¾Ø§Ø³Ø® Ø®Ø§Ù„ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯")
                        
                else:
                    print(f"âŒ Ø®Ø·Ø§ÛŒ HTTP: {response.status_code}")
                    
            except requests.exceptions.Timeout:
                print(f"â° Timeout Ø¯Ø± ØªÙ„Ø§Ø´ {attempt + 1}")
                if attempt < max_retries - 1:
                    print("ğŸ”„ ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯...")
                    await asyncio.sleep(3)  # Ø§ÙØ²Ø§ÛŒØ´ Ø²Ù…Ø§Ù† Ø§Ù†ØªØ¸Ø§Ø±
                    
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® (ØªÙ„Ø§Ø´ {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(2)
        
        print("âŒ ØªÙ…Ø§Ù… ØªÙ„Ø§Ø´â€ŒÙ‡Ø§ Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯")
        return None
    
    def _store_for_learning(self, user_message: str, ai_response: str, context: List[Dict], web_info: Dict = None, learning_prompt: str = None):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡"""
        learning_entry = {
            "timestamp": datetime.now().isoformat(),
            "user_message": user_message,
            "ai_response": ai_response,
            "context_used": len(context) if context else 0,
            "web_search_used": bool(web_info),
            "web_sources": web_info.get('sources', 0) if web_info else 0,
            "learning_prompt": learning_prompt,
            "quality_score": None  # Ø¨Ø¹Ø¯Ø§Ù‹ Ø¨Ø§ feedback Ú©Ø§Ø±Ø¨Ø± Ù¾Ø± Ù…ÛŒâ€ŒØ´ÙˆØ¯
        }
        
        self.learning_data.append(learning_entry)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„
        os.makedirs("data/learning", exist_ok=True)
        with open("data/learning/conversations.jsonl", "a", encoding="utf-8") as f:
            f.write(json.dumps(learning_entry, ensure_ascii=False) + "\n")
        
        # Ø°Ø®ÛŒØ±Ù‡ prompt ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡
        if learning_prompt:
            with open("data/learning/learning_prompts.md", "a", encoding="utf-8") as f:
                f.write(f"\n\n---\n\n{learning_prompt}")
        
        print("ğŸ“š Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    def _generate_fallback_response(self, message: str, web_info: Dict = None) -> str:
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® fallback ÙˆÙ‚ØªÛŒ Ù…Ø¯Ù„ Ú©Ø§Ø± Ù†Ù…ÛŒâ€ŒÚ©Ù†Ø¯"""
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§Ø±Ø¨Ø± Ø§Ø² Ø­Ø§ÙØ¸Ù‡
        user_name = self.personal_ai.owner_name if hasattr(self, 'personal_ai') else None
        
        # Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙˆØ¨ Ø¯Ø§Ø±ÛŒÙ…ØŒ Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§ Ø§ÙˆÙ† Ø¨Ø§Ø´Ù‡
        if web_info and web_info.get('summary'):
            greeting = f"Ø³Ù„Ø§Ù… {user_name}! " if user_name else "Ø³Ù„Ø§Ù…! "
            return f"{greeting}Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§ÛŒÙ†ØªØ±Ù†Øª:\n\n{web_info['summary']}\n\nğŸ“Š Ù…Ù†Ø§Ø¨Ø¹: {web_info.get('sources', 1)} Ù†ØªÛŒØ¬Ù‡ Ø§Ø² Ø§ÛŒÙ†ØªØ±Ù†Øª"
        
        # Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ fallback Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ø³Ø¤Ø§Ù„
        message_lower = message.lower()
        
        import random
        
        # Ø³Ø¤Ø§Ù„Ø§Øª Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§
        if any(word in message_lower for word in ["Ø¯Ù…Ø§", "Ù‡ÙˆØ§", "Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§", "Ø¨Ø§Ø±Ø´", "Ø¨Ø§Ø±Ø§Ù†", "Ø¨Ø±Ù", "Ú¯Ø±Ù…Ø§", "Ø³Ø±Ù…Ø§"]):
            responses = [
                "Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø§Ù„Ø§Ù† Ù†Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ù‚ÛŒÙ‚ Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§ Ø±Ùˆ Ø¨Ù‡Øª Ø¨Ø¯Ù…. Ø¨Ù‡ØªØ±Ù‡ Ø§Ø² Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ§Ø´Ù†Ø§Ø³ÛŒ Ú†Ú© Ú©Ù†ÛŒ! ğŸŒ¤ï¸",
                "Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ù‚ÛŒÙ‚ Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§ØŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ù… Ø§Ø² Ø§Ù¾ Ù‡ÙˆØ§Ø´Ù†Ø§Ø³ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒ ğŸŒ¡ï¸"
            ]
            return random.choice(responses)
        
        # Ø³Ø¤Ø§Ù„Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ
        elif "ØŸ" in message:
            responses = [
                "Ø³Ø¤Ø§Ù„ Ø¬Ø§Ù„Ø¨ÛŒ! Ø¨Ø°Ø§Ø± Ø¨Ø±Ø§Øª ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯Ù… ğŸ¤”",
                "Ø§ÛŒÙ† Ø³Ø¤Ø§Ù„ Ø±Ùˆ Ø¯ÙˆØ³Øª Ø¯Ø§Ø±Ù…! Ú†Ù‡ Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø§Ù„Ø¨ÛŒ ğŸ’­",
                "Ú©Ù†Ø¬Ú©Ø§ÙˆÛŒ Ø®ÙˆØ¨ÛŒÙ‡! Ø§ÛŒÙ† Ú†ÛŒØ²ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒÙ¾Ø±Ø³ÛŒ..."
            ]
            return random.choice(responses)
        
        # Ø³Ù„Ø§Ù… Ùˆ Ø§Ø­ÙˆØ§Ù„â€ŒÙ¾Ø±Ø³ÛŒ
        elif any(word in message_lower for word in ["Ø³Ù„Ø§Ù…", "Ø¯Ø±ÙˆØ¯", "Ú†Ø·ÙˆØ±", "Ø­Ø§Ù„"]):
            if user_name:
                responses = [
                    f"Ø³Ù„Ø§Ù… {user_name}! Ø®ÙˆØ´Ø­Ø§Ù„Ù… Ú©Ù‡ Ø¨Ø§Ù‡Ø§Ù… Ø­Ø±Ù Ù…ÛŒâ€ŒØ²Ù†ÛŒ! ğŸ¦Š",
                    f"Ø¯Ø±ÙˆØ¯ Ø¨Ø± ØªÙˆ {user_name}! Ú†Ø·ÙˆØ±ÛŒØŸ ğŸ˜Š",
                    f"Ø³Ù„Ø§Ù… {user_name} Ø¹Ø²ÛŒØ²! Ø­Ø§Ù„Ù… Ø®ÙˆØ¨Ù‡ØŒ ØªÙˆ Ú†Ø·ÙˆØ±ÛŒØŸ ğŸŒŸ"
                ]
            else:
                responses = [
                    "Ø³Ù„Ø§Ù…! Ø®ÙˆØ´Ø­Ø§Ù„Ù… Ú©Ù‡ Ø¨Ø§Ù‡Ø§Ù… Ø­Ø±Ù Ù…ÛŒâ€ŒØ²Ù†ÛŒ! ğŸ¦Š",
                    "Ø¯Ø±ÙˆØ¯ Ø¨Ø± ØªÙˆ! Ú†Ø·ÙˆØ±ÛŒØŸ ğŸ˜Š",
                    "Ø³Ù„Ø§Ù… Ø¹Ø²ÛŒØ²! Ø­Ø§Ù„Ù… Ø®ÙˆØ¨Ù‡ØŒ ØªÙˆ Ú†Ø·ÙˆØ±ÛŒØŸ ğŸŒŸ"
                ]
            return random.choice(responses)
        
        # Ù¾Ø§Ø³Ø® Ø¹Ù…ÙˆÙ…ÛŒ
        else:
            responses = [
                "Ø®ÙˆØ¨ Ù¾Ø±Ø³ÛŒØ¯ÛŒ! Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø±Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù… ğŸ”",
                "Ø¬Ø§Ù„Ø¨Ù‡! Ø¨Ø°Ø§Ø± Ø¯Ø±Ø¨Ø§Ø±Ù‡â€ŒØ´ ÙÚ©Ø± Ú©Ù†Ù… ğŸ’­",
                "Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø§Ù„Ø¨ÛŒ! Ú†Ù‡ Ú†ÛŒØ² Ø®ÙˆØ¨ÛŒ Ù¾Ø±Ø³ÛŒØ¯ÛŒ ğŸ¤“"
            ]
            return random.choice(responses)
    
    def get_model_status(self) -> Dict:
        """ÙˆØ¶Ø¹ÛŒØª Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
        return {
            "current_model": self.current_model,
            "available_models": self.models,
            "is_loaded": self.is_model_loaded
        }
    
    async def switch_model(self, model_type: str) -> bool:
        """ØªØºÛŒÛŒØ± Ù…Ø¯Ù„"""
        if model_type not in self.models:
            print(f"âŒ Ù†ÙˆØ¹ Ù…Ø¯Ù„ {model_type} Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª")
            return False
        
        new_model = self.models[model_type]
        print(f"ğŸ”„ ØªØºÛŒÛŒØ± Ù…Ø¯Ù„ Ø¨Ù‡: {new_model}")
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù…Ø¯Ù„
        available_models = await self._get_available_models()
        if not any(new_model in available for available in available_models):
            print(f"ğŸ“¥ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ {new_model}...")
            await self._pull_model(new_model)
        
        self.current_model = new_model
        print(f"âœ… Ù…Ø¯Ù„ ØªØºÛŒÛŒØ± ÛŒØ§ÙØª Ø¨Ù‡: {new_model}")
        return True

    async def fine_tune_from_data(self):
        """Fine-tuning Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ø´Ø¯Ù‡"""
        # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ø¹Ø¯Ø§Ù‹ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        print("ğŸ¯ Fine-tuning Ø¯Ø± Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡ Ø§Ø¶Ø§ÙÙ‡ Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯")
        pass
    
    def toggle_web_search(self, enabled: bool = None) -> bool:
        """ÙØ¹Ø§Ù„/ØºÛŒØ±ÙØ¹Ø§Ù„ Ú©Ø±Ø¯Ù† Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆØ¨"""
        if enabled is not None:
            self.web_enabled = enabled
        else:
            self.web_enabled = not self.web_enabled
        
        status = "ÙØ¹Ø§Ù„" if self.web_enabled else "ØºÛŒØ±ÙØ¹Ø§Ù„"
        print(f"ğŸŒ Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆØ¨ {status} Ø´Ø¯")
        return self.web_enabled
    
    def get_web_status(self) -> Dict:
        """ÙˆØ¶Ø¹ÛŒØª Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆØ¨"""
        return {
            "web_enabled": self.web_enabled,
            "internet_connected": self.web_search.is_online() if hasattr(self, 'web_search') else False,
            "search_engines": list(self.web_search.search_engines.keys()) if hasattr(self, 'web_search') else []
        }
    
    def _build_code_analysis_prompt(self, code_analysis: Dict) -> str:
        """Ø³Ø§Ø®Øª prompt Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ú©Ø¯"""
        analysis = code_analysis['analysis']
        original_code = code_analysis['original_code']
        
        prompt = f"""
ğŸ” ØªØ­Ù„ÛŒÙ„ Ú©Ø¯ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡:

Ú©Ø¯ Ø§ØµÙ„ÛŒ:
```{analysis['language']}
{original_code}
```

Ù†ØªØ§ÛŒØ¬ ØªØ­Ù„ÛŒÙ„:
- Ø²Ø¨Ø§Ù† Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ: {analysis['language']}
- ØªØ¹Ø¯Ø§Ø¯ Ø®Ø·ÙˆØ·: {analysis['lines_count']}
- Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ: {analysis['complexity']}
- ØµØ­Øª syntax: {'âœ… ØµØ­ÛŒØ­' if analysis['syntax_valid'] else 'âŒ Ø®Ø·Ø§ Ø¯Ø§Ø±Ø¯'}

"""
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø´Ú©Ù„Ø§Øª
        if analysis['issues']:
            prompt += "ğŸš¨ Ù…Ø´Ú©Ù„Ø§Øª ÛŒØ§ÙØª Ø´Ø¯Ù‡:\n"
            for issue in analysis['issues']:
                prompt += f"- Ø®Ø· {issue['line']}: {issue['message']} ({issue['severity']})\n"
            prompt += "\n"
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª
        if analysis['suggestions']:
            prompt += "ğŸ’¡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ù‡Ø¨ÙˆØ¯:\n"
            for suggestion in analysis['suggestions']:
                prompt += f"- Ø®Ø· {suggestion['line']}: {suggestion['message']}\n"
            prompt += "\n"
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ
        if analysis.get('general_suggestions'):
            prompt += "ğŸ¯ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ:\n"
            for suggestion in analysis['general_suggestions']:
                prompt += f"- {suggestion}\n"
            prompt += "\n"
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ú©Ø¯ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡
        if analysis['fixed_code'] != original_code:
            prompt += f"ğŸ”§ Ú©Ø¯ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡:\n```{analysis['language']}\n{analysis['fixed_code']}\n```\n\n"
        
        prompt += """
Ù„Ø·ÙØ§Ù‹ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ Ù…Ø§Ù‡Ø±:
1. Ú©Ø¯ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù† Ùˆ Ù…Ø´Ú©Ù„Ø§Øª Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ø±Ø§ ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯Ù‡
2. Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ØªØ± Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¨Ø¯Ù‡
3. Ø§Ú¯Ø± Ú©Ø¯ Ø®Ø·Ø§ Ø¯Ø§Ø±Ù‡ØŒ Ù†Ø­ÙˆÙ‡ Ø§ØµÙ„Ø§Ø­ Ø±Ø§ Ø¨Ú¯Ùˆ
4. Ø¨Ù‡ØªØ±ÛŒÙ† practices Ø±Ø§ ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯Ù‡
5. Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ùˆ Ø¨Ù‡ ØµÙˆØ±Øª Ø³Ø§Ø¯Ù‡ ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯Ù‡
"""
        
        return prompt

    def detect_code_in_message(self, message: str) -> bool:
        """ØªØ´Ø®ÛŒØµ ÙˆØ¬ÙˆØ¯ Ú©Ø¯ Ø¯Ø± Ù¾ÛŒØ§Ù…"""
        code_indicators = [
            'def ', 'function', 'class ', 'import ', 'from ',
            'var ', 'let ', 'const ', 'if (', 'for (', 'while (',
            'public class', '#include', 'SELECT', 'INSERT',
            '```', 'Ú©Ø¯', 'Ø¨Ø±Ù†Ø§Ù…Ù‡', 'Ø§Ø³Ú©Ø±ÛŒÙ¾Øª', 'function',
            '{', '}', '()', '=>', '==', '!=', '&&', '||'
        ]
        
        message_lower = message.lower()
        return any(indicator in message_lower for indicator in code_indicators)
    
    def extract_code_from_message(self, message: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø¯ Ø§Ø² Ù¾ÛŒØ§Ù…"""
        # Ø§Ú¯Ø± Ú©Ø¯ Ø¯Ø± ``` Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ù‡
        code_blocks = re.findall(r'```(?:\w+)?\n?(.*?)\n?```', message, re.DOTALL)
        if code_blocks:
            return code_blocks[0].strip()
        
        # Ø§Ú¯Ø± Ú©Ø¯ Ø¯Ø± Ø®Ø·ÙˆØ· Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ Ù‡Ø³Øª
        lines = message.split('\n')
        code_lines = []
        in_code_block = False
        
        for line in lines:
            if any(indicator in line for indicator in ['def ', 'function', 'class ', 'import']):
                in_code_block = True
            
            if in_code_block:
                code_lines.append(line)
                
                # Ø§Ú¯Ø± Ø®Ø· Ø®Ø§Ù„ÛŒ ÛŒØ§ ØºÛŒØ±Ú©Ø¯ Ø¨ÙˆØ¯ØŒ ØªÙˆÙ‚Ù
                if line.strip() == '' or (not any(c in line for c in ['{', '}', '(', ')', '=', ';'])):
                    if len(code_lines) > 1:
                        break
        
        return '\n'.join(code_lines).strip()
    
    def analyze_user_code(self, message: str) -> Optional[Dict]:
        """ØªØ­Ù„ÛŒÙ„ Ú©Ø¯ Ú©Ø§Ø±Ø¨Ø±"""
        if not self.detect_code_in_message(message):
            return None
        
        code = self.extract_code_from_message(message)
        if not code:
            return None
        
        print(f"ğŸ” Ú©Ø¯ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯: {code[:50]}...")
        
        # ØªØ­Ù„ÛŒÙ„ Ú©Ø¯
        analysis = code_analyzer.analyze_code(code)
        
        return {
            'original_code': code,
            'analysis': analysis,
            'has_issues': len(analysis['issues']) > 0,
            'has_suggestions': len(analysis['suggestions']) > 0
        }
    
    def _build_initial_prompt(self, message: str, context: List[Dict] = None, personality: Dict = None, web_info: Dict = None, code_analysis: Dict = None) -> str:
        """Ø³Ø§Ø®Øª prompt Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ AI Ø¨Ø§ context Ù‚ÙˆÛŒâ€ŒØªØ±"""
        
        # Ø§Ø¹Ù…Ø§Ù„ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø´Ø®ØµÛŒ Ø¨Ù‡ Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø±
        processed_message = personal_learning_system.apply_vocabulary_to_message(message)
        if processed_message != message:
            print(f"ğŸ“š ÙˆØ§Ú˜Ú¯Ø§Ù† Ø´Ø®ØµÛŒ Ø§Ø¹Ù…Ø§Ù„ Ø´Ø¯: {message[:30]}... â†’ {processed_message[:30]}...")
        
        # Ø¯Ø±ÛŒØ§ÙØª Ù‚ÙˆØ§Ù†ÛŒÙ† ÙØ¹Ø§Ù„ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† context
        active_rules = personal_learning_system.get_active_rules_for_context(processed_message)
        
        # Ø¯Ø±ÛŒØ§ÙØª ØªØ±Ø¬ÛŒØ­Ø§Øª Ù„Ø­Ù†
        tone_preferences = personal_learning_system.get_tone_preferences()
        
        # Ø³Ø§Ø®Øª prompt Ù¾Ø§ÛŒÙ‡
        current_ai_name = dynamic_name_learning.get_current_name()
        system_prompt = f"""ØªÙˆ {current_ai_name} Ù‡Ø³ØªÛŒØŒ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ ÙØ§Ø±Ø³ÛŒ Ú©Ù‡:
- Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯Ù‡ÛŒ
- Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ùˆ Ù…ÙÛŒØ¯ Ù‡Ø³ØªÛŒ
- Ù†Ø§Ù… ØªÙˆ "{current_ai_name}" Ø§Ø³Øª
- Ù…Ú©Ø§Ù„Ù…Ù‡ Ù‚Ø¨Ù„ÛŒ Ø±Ø§ Ø¨Ù‡ Ø®Ø§Ø·Ø± Ø¯Ø§Ø±ÛŒ Ùˆ Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ Ø¢Ù† Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯Ù‡ÛŒ
- Ø§Ú¯Ø± Ø³Ø¤Ø§Ù„ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù…ÙˆØ¶ÙˆØ¹ Ù‚Ø¨Ù„ÛŒ Ø§Ø³ØªØŒ Ø­ØªÙ…Ø§Ù‹ Ø¨Ù‡ Ø¢Ù† Ø§Ø´Ø§Ø±Ù‡ Ú©Ù†
- Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒØª Ú©ÙˆØªØ§Ù‡ Ùˆ Ù…ÙÛŒØ¯ Ø¨Ø§Ø´Ù†Ø¯ (Ø­Ø¯Ø§Ú©Ø«Ø± 3-4 Ø¬Ù…Ù„Ù‡)
- Ù†Ø§Ù… ØªÙˆ Ù‚Ø§Ø¨Ù„ ØªØºÛŒÛŒØ± Ø§Ø³Øª Ùˆ Ø§Ø² Ù…Ú©Ø§Ù„Ù…Ù‡ ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒ"""
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø³Ø·Ø­ Ø±Ø§Ø¨Ø·Ù‡ (Ø¯Ø³ØªÛŒØ§Ø± Ø´Ø®ØµÛŒ)
        system_prompt += f"\n- Ø³Ø·Ø­ Ø±Ø§Ø¨Ø·Ù‡: {self.personal_ai.relationship_level.name}"
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù‚ÙˆØ§Ù†ÛŒÙ† Ø´Ø®ØµÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø´Ø¯Ù‡
        if active_rules:
            system_prompt += "\n\nÙ‚ÙˆØ§Ù†ÛŒÙ† Ø´Ø®ØµÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø´Ø¯Ù‡:\n"
            for rule in active_rules[:3]:  # Ø­Ø¯Ø§Ú©Ø«Ø± 3 Ù‚Ø§Ù†ÙˆÙ†
                system_prompt += f"- {rule['rule_text']}\n"
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªØ±Ø¬ÛŒØ­Ø§Øª Ù„Ø­Ù†
        if tone_preferences:
            tone_text = ""
            if tone_preferences.get("style"):
                tone_text += f"Ù„Ø­Ù†: {tone_preferences['style']}, "
            if tone_preferences.get("formality"):
                tone_text += f"Ø±Ø³Ù…ÛŒØª: {tone_preferences['formality']}, "
            if tone_preferences.get("response_length"):
                tone_text += f"Ø·ÙˆÙ„ Ù¾Ø§Ø³Ø®: {tone_preferences['response_length']}"
            
            if tone_text:
                system_prompt += f"\nØ³Ø¨Ú© Ù¾Ø§Ø³Ø® Ù…Ø·Ù„ÙˆØ¨: {tone_text.rstrip(', ')}\n"
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø®Ù„Ø§ØµÙ‡ Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ú©Ø§Ø±Ø¨Ø±
        profile_summary = personal_learning_system.get_profile_summary()
        if profile_summary:
            system_prompt += f"\nÙ¾Ø±ÙˆÙØ§ÛŒÙ„ Ú©Ø§Ø±Ø¨Ø± (Ø®Ù„Ø§ØµÙ‡):\n{profile_summary}\n"
        
        # Ø§Ø´Ø§Ø±Ù‡ Ø¨Ù‡ Ù‡Ù…Ø¯Ù„ÛŒ Ø¯Ø± ØµÙˆØ±Øª ØªÙ†Ù‡Ø§ÛŒÛŒ
        try:
            if personal_learning_system.profile.get("social", {}).get("lonely"):
                system_prompt += "\n- Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± Ø§Ø­Ø³Ø§Ø³ ØªÙ†Ù‡Ø§ÛŒÛŒ Ø¯Ø§Ø±Ø¯ØŒ Ù‡Ù…Ø¯Ù„ Ùˆ Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§Ø´"
        except Exception:
            pass
        
        # Ø³Ø§Ø®Øª context Ù‚ÙˆÛŒâ€ŒØªØ± Ø§Ø² Ù…Ú©Ø§Ù„Ù…Ù‡
        conversation_context = self._build_conversation_context(context)
        
        # ØªØ´Ø®ÛŒØµ Ù…ÙˆØ¶ÙˆØ¹ ÙØ¹Ù„ÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡
        current_topic = self._detect_conversation_topic(processed_message, context)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙˆØ¨
        web_text = ""
        if web_info and web_info.get('summary'):
            web_text = f"\n\nØ§Ø·Ù„Ø§Ø¹Ø§Øª Ø¬Ø¯ÛŒØ¯ Ø§Ø² Ø§ÛŒÙ†ØªØ±Ù†Øª:\n{web_info['summary']}\n"
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªØ­Ù„ÛŒÙ„ Ú©Ø¯
        code_text = ""
        if code_analysis:
            code_text = f"\n\nØªØ­Ù„ÛŒÙ„ Ú©Ø¯:\n{self._build_code_analysis_prompt(code_analysis)}\n"
        
        prompt = f"""{system_prompt}

{conversation_context}

Ù…ÙˆØ¶ÙˆØ¹ ÙØ¹Ù„ÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡: {current_topic}
{web_text}
{code_text}

Ú©Ø§Ø±Ø¨Ø±: {processed_message}
{current_ai_name}:"""
        
        return prompt
    
    def _build_conversation_context(self, context: List[Dict] = None) -> str:
        """Ø³Ø§Ø®Øª context Ù‚ÙˆÛŒâ€ŒØªØ± Ø§Ø² Ù…Ú©Ø§Ù„Ù…Ù‡"""
        if not context or len(context) == 0:
            return "Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¬Ø¯ÛŒØ¯ Ø´Ø±ÙˆØ¹ Ø´Ø¯Ù‡ Ø§Ø³Øª."
        
        # Ø§Ú¯Ø± context Ø®ÛŒÙ„ÛŒ Ú©Ù… Ø¨Ø§Ø´Ù‡ (Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØªØºÛŒÛŒØ± Ù…ÙˆØ¶ÙˆØ¹)
        if len(context) <= 1:
            return "Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÛŒØ¯ Ø´Ø±ÙˆØ¹ Ø´Ø¯Ù‡ Ø§Ø³Øª."
        
        # Ú¯Ø±ÙØªÙ† Ø¢Ø®Ø±ÛŒÙ† 6 Ù¾ÛŒØ§Ù… (3 Ø¬ÙØª Ø³Ø¤Ø§Ù„ Ùˆ Ø¬ÙˆØ§Ø¨)
        recent_messages = context[-6:] if len(context) >= 6 else context
        
        conversation_text = "ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡:\n"
        
        for i, item in enumerate(recent_messages):
            role = "Ú©Ø§Ø±Ø¨Ø±" if item.get('role') == 'user' else "Ø±ÙˆØ¨Ø§Ù‡"
            content = item.get('content', '')
            
            # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„ Ù‡Ø± Ù¾ÛŒØ§Ù…
            if len(content) > 150:
                content = content[:150] + "..."
            
            conversation_text += f"{role}: {content}\n"
        
        # ØªØ´Ø®ÛŒØµ Ø§Ù„Ú¯ÙˆÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡
        conversation_pattern = self._analyze_conversation_pattern(recent_messages)
        if conversation_pattern:
            conversation_text += f"\nØ§Ù„Ú¯ÙˆÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡: {conversation_pattern}\n"
        
        return conversation_text
    
    def _personalize_final_response(self, message: str, response: str, analysis: Dict) -> str:
        """Ø§Ø¹Ù…Ø§Ù„ ØªØ±Ø¬ÛŒØ­Ø§Øª Ùˆ Ù„Ø­Ù† Ø´Ø®ØµÛŒ Ø±ÙˆÛŒ Ù¾Ø§Ø³Ø® Ù†Ù‡Ø§ÛŒÛŒ"""
        personalized = response or ""
        
        # Ø§Ø¹Ù…Ø§Ù„ ØªØ±Ø¬ÛŒØ­Ø§Øª Ø·ÙˆÙ„ Ù¾Ø§Ø³Ø®
        tone_preferences = personal_learning_system.get_tone_preferences()
        response_length = tone_preferences.get("response_length")
        if response_length == "Ú©ÙˆØªØ§Ù‡":
            personalized = self._truncate_response(personalized, max_sentences=2, max_chars=280)
        
        # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…Ø«Ø§Ù„ Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± ØªØ±Ø¬ÛŒØ­ Ø¯Ø§Ø¯Ù‡
        if tone_preferences.get("include_examples"):
            if self._is_explanation_request(message) and "Ù…Ø«Ø§Ù„" not in personalized:
                personalized += "\nØ§Ú¯Ø± Ø¨Ø®ÙˆØ§Ù‡ÛŒ Ø¨Ø§ ÛŒÚ© Ù…Ø«Ø§Ù„ Ø¹Ù…Ù„ÛŒ Ù‡Ù… ØªÙˆØ¶ÛŒØ­ Ù…ÛŒâ€ŒØ¯Ù‡Ù…."
        
        # Ù„Ù…Ø³ Ù‡Ù…â€ŒÙ†Ø´ÛŒÙ†ÛŒ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø± ØªÙ†Ù‡Ø§
        try:
            lonely_flag = personal_learning_system.profile.get("social", {}).get("lonely")
            if lonely_flag and self.personal_ai.should_add_companion_note():
                personalized += "\nÙ…Ù† Ø§ÛŒÙ†Ø¬Ø§Ù… Ùˆ Ú©Ù†Ø§Ø±ØªÙ…."
                self.personal_ai.mark_companion_note_used()
        except Exception:
            pass
        
        return personalized.strip()
    
    def _is_explanation_request(self, message: str) -> bool:
        """ØªØ´Ø®ÛŒØµ Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØªÙˆØ¶ÛŒØ­/Ø¢Ù…ÙˆØ²Ø´"""
        indicators = ["Ú†Ø·ÙˆØ±", "Ú†Ú¯ÙˆÙ†Ù‡", "Ø±Ø§Ù‡", "Ø±ÙˆØ´", "ØªÙˆØ¶ÛŒØ­", "ÛŒØ§Ø¯ Ø¨Ø¯Ù‡", "Ø¢Ù…ÙˆØ²Ø´"]
        msg = message.lower()
        return any(word in msg for word in indicators)
    
    def _truncate_response(self, text: str, max_sentences: int = 2, max_chars: int = 280) -> str:
        """Ú©ÙˆØªØ§Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø§Ø³Ø® Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ Ø¬Ù…Ù„Ù‡/Ú©Ø§Ø±Ø§Ú©ØªØ±"""
        if not text:
            return text
        
        # Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡ Ø¬Ù…Ù„Ø§Øª ÙØ§Ø±Ø³ÛŒ/Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
        sentence_enders = ["ØŸ", "!", ".", "â€¦", "ØŸ", "Ø›", "\n"]
        sentences = []
        current = []
        for ch in text:
            current.append(ch)
            if ch in sentence_enders:
                sentence = "".join(current).strip()
                if sentence:
                    sentences.append(sentence)
                current = []
        if current:
            sentences.append("".join(current).strip())
        
        if sentences:
            text = " ".join(sentences[:max_sentences]).strip()
        
        if len(text) > max_chars:
            text = text[:max_chars].rstrip()
            if not text.endswith(("â€¦", ".", "!", "ØŸ")):
                text += "â€¦"
        
        return text
    
    def _detect_conversation_topic(self, current_message: str, context: List[Dict] = None) -> str:
        """ØªØ´Ø®ÛŒØµ Ù…ÙˆØ¶ÙˆØ¹ ÙØ¹Ù„ÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡ - Ø¯Ø§ÛŒÙ†Ø§Ù…ÛŒÚ© Ùˆ ÛŒØ§Ø¯Ú¯ÛŒØ±Ù†Ø¯Ù‡"""
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØªØºÛŒÛŒØ± Ù…ÙˆØ¶ÙˆØ¹
        if self._is_topic_change_request(current_message):
            print("ğŸ”„ Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØªØºÛŒÛŒØ± Ù…ÙˆØ¶ÙˆØ¹ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯")
            # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ù…ÙˆØ¶ÙˆØ¹ ÙØ¹Ù„ÛŒ
            self.current_conversation_topic = None
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÛŒØ¯ Ø§Ø² Ù¾ÛŒØ§Ù…
            new_topic = self._extract_dynamic_topic(current_message)
            return new_topic if new_topic != "Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¹Ù…ÙˆÙ…ÛŒ" else "Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÛŒØ¯"
        
        # Ø§Ú¯Ø± context Ù†Ø¯Ø§Ø±ÛŒÙ…ØŒ Ù…ÙˆØ¶ÙˆØ¹ Ø±Ùˆ Ø§Ø² Ù¾ÛŒØ§Ù… ÙØ¹Ù„ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†
        if not context or len(context) == 0:
            return self._extract_dynamic_topic(current_message)
        
        # Ø§Ø¨ØªØ¯Ø§ Ù…ÙˆØ¶ÙˆØ¹ Ù¾ÛŒØ§Ù… ÙØ¹Ù„ÛŒ Ø±Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù† (Ù†Ù‡ Ú©Ù„ context)
        current_topic_from_message = self._extract_dynamic_topic(current_message)
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù…ÙˆØ¶ÙˆØ¹ Ø§Ø² ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø´Ø¯Ù‡â€ŒÙ‡Ø§
        learned_topic = self._detect_learned_topic(current_message)
        if learned_topic:
            return learned_topic
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ù…ÙˆØ¶ÙˆØ¹ Ø§Ø² Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù¾Ø§ÛŒÙ‡
        static_topic = self._detect_static_topic(current_message)
        if static_topic != "Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¹Ù…ÙˆÙ…ÛŒ":
            return static_topic
        
        # Ø§Ú¯Ø± Ù‡ÛŒÚ† Ù…ÙˆØ¶ÙˆØ¹ Ø´Ù†Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡â€ŒØ§ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÛŒØ¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†
        new_topic = current_topic_from_message
        
        # Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÛŒØ¯ Ø±Ùˆ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±
        if new_topic != "Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¹Ù…ÙˆÙ…ÛŒ":
            self._learn_new_topic(new_topic, current_message)
        
        return new_topic
    
    def _detect_static_topic(self, text: str) -> str:
        """ØªØ´Ø®ÛŒØµ Ù…ÙˆØ¶ÙˆØ¹ Ø§Ø² Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù¾Ø§ÛŒÙ‡"""
        text_lower = text.lower()
        
        # Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù¾Ø§ÛŒÙ‡
        static_topics = {
            "ÙˆØ±Ø²Ø´": ["ÙÙˆØªØ¨Ø§Ù„", "Ø¨Ø³Ú©ØªØ¨Ø§Ù„", "ÙˆØ§Ù„ÛŒØ¨Ø§Ù„", "ØªÙ†ÛŒØ³", "Ø´Ù†Ø§", "Ø¨Ø§Ø²ÛŒ", "Ù…Ø³Ø§Ø¨Ù‚Ù‡", "ØªÛŒÙ…", "ÙˆØ±Ø²Ø´Ú©Ø§Ø±", "Ú¯Ù„", "Ø§Ù…ØªÛŒØ§Ø²"],
            "Ù…ÙˆØ³ÛŒÙ‚ÛŒ": ["Ø¢Ù‡Ù†Ú¯", "Ø®ÙˆØ§Ù†Ù†Ø¯Ù‡", "Ø³Ø§Ø²", "Ù…ÙˆØ²ÛŒÚ©", "Ú©Ù†Ø³Ø±Øª", "Ø¢Ù„Ø¨ÙˆÙ…", "ØªØ±Ø§Ù†Ù‡", "Ù†ÙˆØ§Ø²Ù†Ø¯Ù‡"],
            "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ": ["Ú©Ø¯", "Ø¨Ø±Ù†Ø§Ù…Ù‡", "python", "javascript", "html", "css", "function", "variable", "loop", "ØªØ§Ø¨Ø¹", "Ù…ØªØºÛŒØ±"],
            "Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§": ["Ø¯Ù…Ø§", "Ù‡ÙˆØ§", "Ø¨Ø§Ø±Ø´", "Ø¨Ø§Ø±Ø§Ù†", "Ø¨Ø±Ù", "Ø¢ÙØªØ§Ø¨ÛŒ", "Ø§Ø¨Ø±ÛŒ", "Ú¯Ø±Ù…Ø§", "Ø³Ø±Ù…Ø§", "Ù‡ÙˆØ§Ø´Ù†Ø§Ø³ÛŒ"],
            "Ø¢Ø´Ù¾Ø²ÛŒ": ["ØºØ°Ø§", "Ù¾Ø®Øª", "Ø¯Ø³ØªÙˆØ±", "Ù…ÙˆØ§Ø¯", "Ø·Ø¨Ø®", "Ø¢Ø´Ù¾Ø²ÛŒ", "Ø®ÙˆØ±Ø§Ú©", "Ø·Ø¹Ø§Ù…"],
            "Ø³ÙØ±": ["Ø³ÙØ±", "Ù…Ø³Ø§ÙØ±Øª", "Ø´Ù‡Ø±", "Ú©Ø´ÙˆØ±", "Ù‡ØªÙ„", "Ø¨Ù„ÛŒØ·", "Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒ", "Ø¬Ø§Ù‡Ø§ÛŒ Ø¯ÛŒØ¯Ù†ÛŒ"],
            "ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ": ["Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±", "Ù…ÙˆØ¨Ø§ÛŒÙ„", "Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù†", "Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø±", "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ", "AI", "ÙÙ†Ø§ÙˆØ±ÛŒ"],
            "Ø³Ù„Ø§Ù…ØªÛŒ": ["Ø³Ù„Ø§Ù…Øª", "Ø¨ÛŒÙ…Ø§Ø±ÛŒ", "Ø¯Ú©ØªØ±", "Ø¯Ø§Ø±Ùˆ", "ÙˆØ±Ø²Ø´", "ØªØºØ°ÛŒÙ‡", "Ø¨Ù‡Ø¯Ø§Ø´Øª"],
            "ØªØ­ØµÛŒÙ„": ["Ø¯Ø±Ø³", "Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡", "Ù…Ø¯Ø±Ø³Ù‡", "Ø§Ù…ØªØ­Ø§Ù†", "ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ", "Ú©ØªØ§Ø¨", "Ù…Ø·Ø§Ù„Ø¹Ù‡"]
        }
        
        # Ø§Ù…ØªÛŒØ§Ø²Ø¯Ù‡ÛŒ Ø¨Ù‡ Ù‡Ø± Ù…ÙˆØ¶ÙˆØ¹
        topic_scores = {}
        for topic, keywords in static_topics.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            if score > 0:
                topic_scores[topic] = score
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÙˆØ¶ÙˆØ¹ Ø¨Ø§ Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø²
        if topic_scores:
            best_topic = max(topic_scores, key=topic_scores.get)
            return best_topic
        
        return "Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¹Ù…ÙˆÙ…ÛŒ"
    
    def _detect_learned_topic(self, text: str) -> Optional[str]:
        """ØªØ´Ø®ÛŒØµ Ù…ÙˆØ¶ÙˆØ¹ Ø§Ø² Ù…ÙˆØ¶ÙˆØ¹Ø§Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø´Ø¯Ù‡"""
        learned_topics = self._load_learned_topics()
        
        if not learned_topics:
            return None
        
        text_lower = text.lower()
        best_topic = None
        best_score = 0
        
        for topic_name, topic_data in learned_topics.items():
            keywords = topic_data.get('keywords', [])
            score = sum(1 for keyword in keywords if keyword in text_lower)
            
            if score > best_score:
                best_score = score
                best_topic = topic_name
        
        # Ø­Ø¯Ø§Ù‚Ù„ 2 Ú©Ù„Ù…Ù‡ Ù…Ø·Ø§Ø¨Ù‚Øª Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù‡
        return best_topic if best_score >= 2 else None
    
    def _extract_dynamic_topic(self, text: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÛŒØ¯ Ø§Ø² Ù…ØªÙ† Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø§ÛŒÙ†Ø§Ù…ÛŒÚ©"""
        import re
        
        text_lower = text.lower()
        
        # Ø­Ø°Ù Ú©Ù„Ù…Ø§Øª Ø±Ø§ÛŒØ¬ Ùˆ ØºÛŒØ±Ù…ÙÛŒØ¯
        stop_words = [
            "Ù…Ù†", "ØªÙˆ", "Ø§Ùˆ", "Ù…Ø§", "Ø´Ù…Ø§", "Ø¢Ù†â€ŒÙ‡Ø§", "Ø§ÛŒÙ†", "Ø¢Ù†", "Ú©Ù‡", "Ø±Ø§", "Ø¨Ù‡", "Ø§Ø²", "Ø¯Ø±", "Ø¨Ø§", "Ø¨Ø±Ø§ÛŒ",
            "Ùˆ", "ÛŒØ§", "Ø§Ù…Ø§", "Ú†ÙˆÙ†", "Ø§Ú¯Ø±", "ÙˆÙ‚ØªÛŒ", "Ú©Ø¬Ø§", "Ú†Ù‡", "Ú†Ø±Ø§", "Ú†Ø·ÙˆØ±", "Ú©ÛŒ", "Ú†Ù†Ø¯", "Ú†Ù‚Ø¯Ø±",
            "Ù…ÛŒâ€ŒØ®ÙˆØ§Ù…", "Ù…ÛŒâ€ŒØªÙˆÙ†Ù…", "Ù…ÛŒâ€ŒØ´Ù‡", "Ø¨Ø§ÛŒØ¯", "Ù†Ø¨Ø§ÛŒØ¯", "Ø¯Ø§Ø±Ù…", "Ù†Ø¯Ø§Ø±Ù…", "Ù‡Ø³Øª", "Ù†ÛŒØ³Øª",
            "Ø®ÛŒÙ„ÛŒ", "Ú©Ù…ÛŒ", "Ø²ÛŒØ§Ø¯", "Ú©Ù…", "Ø¨ÛŒØ´ØªØ±", "Ú©Ù…ØªØ±", "Ù‡Ù…Ù‡", "Ù‡ÛŒÚ†", "ÛŒÚ©", "Ø¯Ùˆ", "Ø³Ù‡",
            "Ø³Ù„Ø§Ù…", "Ø¯Ø±ÙˆØ¯", "Ø¨ÛŒØ§", "Ø¯Ø±Ø¨Ø§Ø±Ù‡", "Ø­Ø±Ù", "Ø¨Ø²Ù†ÛŒÙ…", "ØµØ­Ø¨Øª", "Ú©Ù†ÛŒÙ…", "Ø¢ÛŒØ§", "Ú©Ø¯Ø§Ù…",
            "Ø¯ÙˆØ³Øª", "Ø¯Ø§Ø±ÛŒ", "Ø¢Ø®Ø±ÛŒÙ†", "Ú¯ÙˆØ´", "Ø¯Ø§Ø¯ÛŒ", "Ø¨ÙˆØ¯", "Ú†ÛŒ", "Ø³Ø¤Ø§Ù„", "Ø¯Ø§Ø±Ù…", "ÛŒØ§Ø¯", "Ø¨Ú¯ÛŒØ±Ù…"
        ]
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ù…Ù‡Ù… (Ø§Ø³Ø§Ù…ÛŒØŒ ØµÙØ§ØªØŒ Ø§ÙØ¹Ø§Ù„ Ù…Ù‡Ù…)
        words = re.findall(r'[Ø¢-ÛŒ]+', text_lower)
        important_words = [word for word in words if len(word) > 3 and word not in stop_words]
        
        if not important_words:
            return "Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¹Ù…ÙˆÙ…ÛŒ"
        
        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø´Ù†Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡
        topic_indicators = {
            "ÙÙˆØªØ¨Ø§Ù„": "ÙˆØ±Ø²Ø´", "Ø¨Ø³Ú©ØªØ¨Ø§Ù„": "ÙˆØ±Ø²Ø´", "ÙˆØ§Ù„ÛŒØ¨Ø§Ù„": "ÙˆØ±Ø²Ø´", "ØªÙ†ÛŒØ³": "ÙˆØ±Ø²Ø´", "Ø´Ù†Ø§": "ÙˆØ±Ø²Ø´",
            "Ø¨Ø§Ø²ÛŒ": "ÙˆØ±Ø²Ø´", "Ù…Ø³Ø§Ø¨Ù‚Ù‡": "ÙˆØ±Ø²Ø´", "ØªÛŒÙ…": "ÙˆØ±Ø²Ø´", "ÙˆØ±Ø²Ø´Ú©Ø§Ø±": "ÙˆØ±Ø²Ø´",
            
            "Ø¢Ù‡Ù†Ú¯": "Ù…ÙˆØ³ÛŒÙ‚ÛŒ", "Ù…ÙˆØ²ÛŒÚ©": "Ù…ÙˆØ³ÛŒÙ‚ÛŒ", "Ø³Ø§Ø²": "Ù…ÙˆØ³ÛŒÙ‚ÛŒ", "Ø®ÙˆØ§Ù†Ù†Ø¯Ù‡": "Ù…ÙˆØ³ÛŒÙ‚ÛŒ",
            "Ú©Ù†Ø³Ø±Øª": "Ù…ÙˆØ³ÛŒÙ‚ÛŒ", "Ø¢Ù„Ø¨ÙˆÙ…": "Ù…ÙˆØ³ÛŒÙ‚ÛŒ", "ØªØ±Ø§Ù†Ù‡": "Ù…ÙˆØ³ÛŒÙ‚ÛŒ", "Ù†ÙˆØ§Ø²Ù†Ø¯Ù‡": "Ù…ÙˆØ³ÛŒÙ‚ÛŒ",
            
            "ÙÛŒÙ„Ù…": "Ø³ÛŒÙ†Ù…Ø§", "Ø³Ø±ÛŒØ§Ù„": "Ø³ÛŒÙ†Ù…Ø§", "Ø¨Ø§Ø²ÛŒÚ¯Ø±": "Ø³ÛŒÙ†Ù…Ø§", "Ú©Ø§Ø±Ú¯Ø±Ø¯Ø§Ù†": "Ø³ÛŒÙ†Ù…Ø§",
            "Ø³ÛŒÙ†Ù…Ø§": "Ø³ÛŒÙ†Ù…Ø§", "Ù†Ù…Ø§ÛŒØ´": "Ø³ÛŒÙ†Ù…Ø§",
            
            "python": "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ", "javascript": "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ", "Ú©Ø¯": "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ",
            "Ø¨Ø±Ù†Ø§Ù…Ù‡": "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ", "ØªØ§Ø¨Ø¹": "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ", "Ù…ØªØºÛŒØ±": "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ",
            
            "ØºØ°Ø§": "Ø¢Ø´Ù¾Ø²ÛŒ", "Ù¾Ø®Øª": "Ø¢Ø´Ù¾Ø²ÛŒ", "Ø¢Ø´Ù¾Ø²ÛŒ": "Ø¢Ø´Ù¾Ø²ÛŒ", "Ø·Ø¨Ø®": "Ø¢Ø´Ù¾Ø²ÛŒ",
            "Ø®ÙˆØ±Ø§Ú©": "Ø¢Ø´Ù¾Ø²ÛŒ", "Ø·Ø¹Ø§Ù…": "Ø¢Ø´Ù¾Ø²ÛŒ", "Ø¯Ø³ØªÙˆØ±": "Ø¢Ø´Ù¾Ø²ÛŒ",
            
            "Ø³ÙØ±": "Ø³ÙØ±", "Ù…Ø³Ø§ÙØ±Øª": "Ø³ÙØ±", "Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒ": "Ø³ÙØ±", "Ù‡ØªÙ„": "Ø³ÙØ±",
            "Ø¨Ù„ÛŒØ·": "Ø³ÙØ±", "Ø´Ù‡Ø±": "Ø³ÙØ±", "Ú©Ø´ÙˆØ±": "Ø³ÙØ±"
        }
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù„Ù…Ø§Øª Ù…Ù‡Ù… Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ù…ÙˆØ¶ÙˆØ¹
        for word in important_words:
            if word in topic_indicators:
                return topic_indicators[word]
        
        # Ø§Ú¯Ø± Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø´Ø®ØµÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ø§Ø² Ø§ÙˆÙ„ÛŒÙ† Ú©Ù„Ù…Ù‡ Ù…Ù‡Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        if important_words:
            first_important = important_words[0]
            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù…ÙˆØ¶ÙˆØ¹ Ù…Ù†Ø§Ø³Ø¨
            return self._normalize_topic_name(first_important, important_words[:3])
        
        return "Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¹Ù…ÙˆÙ…ÛŒ"
    
    def _normalize_topic_name(self, main_word: str, context_words: List[str]) -> str:
        """ØªØ¨Ø¯ÛŒÙ„ Ú©Ù„Ù…Ù‡ Ø§ØµÙ„ÛŒ Ø¨Ù‡ Ù†Ø§Ù… Ù…ÙˆØ¶ÙˆØ¹ Ù…Ù†Ø§Ø³Ø¨"""
        
        # Ù‚ÙˆØ§Ù†ÛŒÙ† ØªØ¨Ø¯ÛŒÙ„ Ú©Ù„Ù…Ø§Øª Ø¨Ù‡ Ù…ÙˆØ¶ÙˆØ¹
        topic_mappings = {
            # ÙˆØ±Ø²Ø´
            "ÙÙˆØªØ¨Ø§Ù„": "ÙˆØ±Ø²Ø´", "Ø¨Ø³Ú©ØªØ¨Ø§Ù„": "ÙˆØ±Ø²Ø´", "ÙˆØ§Ù„ÛŒØ¨Ø§Ù„": "ÙˆØ±Ø²Ø´", "ØªÙ†ÛŒØ³": "ÙˆØ±Ø²Ø´", "Ø´Ù†Ø§": "ÙˆØ±Ø²Ø´",
            "Ø¨Ø§Ø²ÛŒ": "ÙˆØ±Ø²Ø´", "Ù…Ø³Ø§Ø¨Ù‚Ù‡": "ÙˆØ±Ø²Ø´", "ØªÛŒÙ…": "ÙˆØ±Ø²Ø´", "ÙˆØ±Ø²Ø´Ú©Ø§Ø±": "ÙˆØ±Ø²Ø´",
            # Ù…ÙˆØ³ÛŒÙ‚ÛŒ
            "Ø¢Ù‡Ù†Ú¯": "Ù…ÙˆØ³ÛŒÙ‚ÛŒ", "Ù…ÙˆØ²ÛŒÚ©": "Ù…ÙˆØ³ÛŒÙ‚ÛŒ", "Ø³Ø§Ø²": "Ù…ÙˆØ³ÛŒÙ‚ÛŒ", "Ø®ÙˆØ§Ù†Ù†Ø¯Ù‡": "Ù…ÙˆØ³ÛŒÙ‚ÛŒ",
            "Ú©Ù†Ø³Ø±Øª": "Ù…ÙˆØ³ÛŒÙ‚ÛŒ", "Ø¢Ù„Ø¨ÙˆÙ…": "Ù…ÙˆØ³ÛŒÙ‚ÛŒ", "ØªØ±Ø§Ù†Ù‡": "Ù…ÙˆØ³ÛŒÙ‚ÛŒ", "Ù†ÙˆØ§Ø²Ù†Ø¯Ù‡": "Ù…ÙˆØ³ÛŒÙ‚ÛŒ",
            # ÙÛŒÙ„Ù… Ùˆ Ø³ÛŒÙ†Ù…Ø§
            "ÙÛŒÙ„Ù…": "Ø³ÛŒÙ†Ù…Ø§", "Ø³Ø±ÛŒØ§Ù„": "Ø³ÛŒÙ†Ù…Ø§", "Ø¨Ø§Ø²ÛŒÚ¯Ø±": "Ø³ÛŒÙ†Ù…Ø§", "Ú©Ø§Ø±Ú¯Ø±Ø¯Ø§Ù†": "Ø³ÛŒÙ†Ù…Ø§",
            "Ø³ÛŒÙ†Ù…Ø§": "Ø³ÛŒÙ†Ù…Ø§", "Ù†Ù…Ø§ÛŒØ´": "Ø³ÛŒÙ†Ù…Ø§",
            # Ú©ØªØ§Ø¨ Ùˆ Ø§Ø¯Ø¨ÛŒØ§Øª
            "Ú©ØªØ§Ø¨": "Ø§Ø¯Ø¨ÛŒØ§Øª", "Ø±Ù…Ø§Ù†": "Ø§Ø¯Ø¨ÛŒØ§Øª", "Ø´Ø¹Ø±": "Ø§Ø¯Ø¨ÛŒØ§Øª", "Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡": "Ø§Ø¯Ø¨ÛŒØ§Øª",
            # Ø®Ø±ÛŒØ¯
            "Ø®Ø±ÛŒØ¯": "Ø®Ø±ÛŒØ¯", "ÙØ±ÙˆØ´Ú¯Ø§Ù‡": "Ø®Ø±ÛŒØ¯", "Ù‚ÛŒÙ…Øª": "Ø®Ø±ÛŒØ¯", "Ù¾ÙˆÙ„": "Ø®Ø±ÛŒØ¯",
            # Ú©Ø§Ø± Ùˆ Ø´ØºÙ„
            "Ú©Ø§Ø±": "Ø´ØºÙ„", "Ø´Ø±Ú©Øª": "Ø´ØºÙ„", "Ù…Ø¯ÛŒØ±": "Ø´ØºÙ„", "Ø­Ù‚ÙˆÙ‚": "Ø´ØºÙ„", "Ø§Ø³ØªØ®Ø¯Ø§Ù…": "Ø´ØºÙ„",
            # Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ
            "python": "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ", "javascript": "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ", "Ú©Ø¯": "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ",
            "Ø¨Ø±Ù†Ø§Ù…Ù‡": "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ", "ØªØ§Ø¨Ø¹": "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ", "Ù…ØªØºÛŒØ±": "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ"
        }
        
        # Ø¨Ø±Ø±Ø³ÛŒ mapping Ù…Ø³ØªÙ‚ÛŒÙ…
        if main_word in topic_mappings:
            return topic_mappings[main_word]
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù„Ù…Ø§Øª context
        for word in context_words:
            if word in topic_mappings:
                return topic_mappings[word]
        
        # Ø§Ú¯Ø± mapping Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ù…ÙˆØ¶ÙˆØ¹ Ø¹Ù…ÙˆÙ…ÛŒ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†
        return "Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¹Ù…ÙˆÙ…ÛŒ"
    
    def _learn_new_topic(self, topic_name: str, text: str):
        """ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÛŒØ¯"""
        if topic_name == "Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¹Ù…ÙˆÙ…ÛŒ":
            return
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø² Ù…ØªÙ†
        import re
        
        text_lower = text.lower()
        stop_words = [
            "Ù…Ù†", "ØªÙˆ", "Ø§Ùˆ", "Ù…Ø§", "Ø´Ù…Ø§", "Ø¢Ù†â€ŒÙ‡Ø§", "Ø§ÛŒÙ†", "Ø¢Ù†", "Ú©Ù‡", "Ø±Ø§", "Ø¨Ù‡", "Ø§Ø²", "Ø¯Ø±", "Ø¨Ø§", "Ø¨Ø±Ø§ÛŒ",
            "Ùˆ", "ÛŒØ§", "Ø§Ù…Ø§", "Ú†ÙˆÙ†", "Ø§Ú¯Ø±", "ÙˆÙ‚ØªÛŒ", "Ú©Ø¬Ø§", "Ú†Ù‡", "Ú†Ø±Ø§", "Ú†Ø·ÙˆØ±", "Ú©ÛŒ", "Ú†Ù†Ø¯", "Ú†Ù‚Ø¯Ø±"
        ]
        
        words = re.findall(r'[Ø¢-ÛŒ]+', text_lower)
        keywords = [word for word in words if len(word) > 2 and word not in stop_words]
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø´Ø¯Ù‡
        learned_topics = self._load_learned_topics()
        
        # Ø§Ú¯Ø± Ù…ÙˆØ¶ÙˆØ¹ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù‡ØŒ Ú©Ù„Ù…Ø§Øª Ø¬Ø¯ÛŒØ¯ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
        if topic_name in learned_topics:
            existing_keywords = set(learned_topics[topic_name]['keywords'])
            new_keywords = set(keywords)
            combined_keywords = list(existing_keywords.union(new_keywords))
            learned_topics[topic_name]['keywords'] = combined_keywords
            learned_topics[topic_name]['usage_count'] += 1
        else:
            # Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÛŒØ¯ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†
            learned_topics[topic_name] = {
                'keywords': keywords[:10],  # Ø­Ø¯Ø§Ú©Ø«Ø± 10 Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ
                'created_at': datetime.now().isoformat(),
                'usage_count': 1
            }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø´Ø¯Ù‡
        self._save_learned_topics(learned_topics)
        
        print(f"ğŸ§  Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÛŒØ¯ ÛŒØ§Ø¯ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯: {topic_name} Ø¨Ø§ {len(keywords)} Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ")
    
    def _load_learned_topics(self) -> Dict:
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø´Ø¯Ù‡"""
        topics_file = "data/learning/learned_topics.json"
        
        if os.path.exists(topics_file):
            try:
                with open(topics_file, "r", encoding="utf-8") as f:
                    return json.load(f)
            except:
                return {}
        
        return {}
    
    def _save_learned_topics(self, topics: Dict):
        """Ø°Ø®ÛŒØ±Ù‡ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø´Ø¯Ù‡"""
        topics_file = "data/learning/learned_topics.json"
        os.makedirs("data/learning", exist_ok=True)
        
        with open(topics_file, "w", encoding="utf-8") as f:
            json.dump(topics, f, ensure_ascii=False, indent=2)
    
    def _extract_topic_from_message(self, message: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÙˆØ¶ÙˆØ¹ Ø§Ø² ÛŒÚ© Ù¾ÛŒØ§Ù…"""
        message_lower = message.lower()
        
        if any(word in message_lower for word in ["Ú©Ø¯", "Ø¨Ø±Ù†Ø§Ù…Ù‡", "python", "javascript"]):
            return "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ"
        elif any(word in message_lower for word in ["Ø¯Ù…Ø§", "Ù‡ÙˆØ§", "Ø¨Ø§Ø±Ø´"]):
            return "Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§"
        elif any(word in message_lower for word in ["ØºØ°Ø§", "Ù¾Ø®Øª", "Ø¢Ø´Ù¾Ø²ÛŒ"]):
            return "Ø¢Ø´Ù¾Ø²ÛŒ"
        elif any(word in message_lower for word in ["Ø³ÙØ±", "Ù…Ø³Ø§ÙØ±Øª", "Ø´Ù‡Ø±"]):
            return "Ø³ÙØ±"
        elif any(word in message_lower for word in ["Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±", "Ù…ÙˆØ¨Ø§ÛŒÙ„", "Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù†"]):
            return "ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ"
        else:
            return "Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¹Ù…ÙˆÙ…ÛŒ"
    
    def _analyze_conversation_pattern(self, messages: List[Dict]) -> str:
        """ØªØ­Ù„ÛŒÙ„ Ø§Ù„Ú¯ÙˆÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡"""
        if len(messages) < 2:
            return None
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
        user_messages = [msg for msg in messages if msg.get('role') == 'user']
        
        if len(user_messages) >= 2:
            # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø¤Ø§Ù„Ø§Øª Ù¾ÛŒ Ø¯Ø± Ù¾ÛŒ
            questions = sum(1 for msg in user_messages if 'ØŸ' in msg.get('content', ''))
            if questions >= 2:
                return "Ø³Ø¤Ø§Ù„ Ùˆ Ù¾Ø§Ø³Ø® Ù…ØªÙˆØ§Ù„ÛŒ"
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØªÙˆØ¶ÛŒØ­ Ø¨ÛŒØ´ØªØ±
            follow_up_words = ["Ø¨ÛŒØ´ØªØ±", "ØªÙˆØ¶ÛŒØ­", "Ø§Ø¯Ø§Ù…Ù‡", "Ú†Ø·ÙˆØ±", "Ú†Ø±Ø§", "Ù…Ø«Ø§Ù„"]
            last_message = user_messages[-1].get('content', '').lower()
            if any(word in last_message for word in follow_up_words):
                return "Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØªÙˆØ¶ÛŒØ­ Ø¨ÛŒØ´ØªØ±"
        
        return "Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¹Ø§Ø¯ÛŒ"
    
    async def _enhance_response_with_datasets(self, message: str, initial_response: str, analysis: Dict, web_info: Dict = None, code_analysis: Dict = None) -> str:
        """Ø¨Ù‡Ø¨ÙˆØ¯ Ù¾Ø§Ø³Ø® Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² dataset Ù‡Ø§"""
        
        # Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø§Ø¨Ù‡ Ø§Ø² dataset
        similar_responses = self.dataset_manager.get_similar_responses(message, analysis)
        
        # Ø¯Ø±ÛŒØ§ÙØª Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡ Ù…Ø±ØªØ¨Ø·
        conversation_patterns = self.dataset_manager.get_conversation_patterns(analysis)
        
        # Ø§Ú¯Ø± dataset Ù‡Ø§ÛŒ Ù…ÙÛŒØ¯ÛŒ Ù¾ÛŒØ¯Ø§ Ø´Ø¯ØŒ Ù¾Ø§Ø³Ø® Ø±Ùˆ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¨Ø¯Ù‡ (Ø¨Ø¯ÙˆÙ† AI Ù…Ø¯Ù„)
        if similar_responses or conversation_patterns:
            print(f"ğŸ“Š Ù¾ÛŒØ¯Ø§ Ø´Ø¯: {len(similar_responses)} Ù¾Ø§Ø³Ø® Ù…Ø´Ø§Ø¨Ù‡ØŒ {len(conversation_patterns)} Ø§Ù„Ú¯Ùˆ")
            
            # Ø¨Ù‡Ø¨ÙˆØ¯ Ø³Ø§Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù„Ú¯ÙˆÙ‡Ø§
            enhanced_response = initial_response
            
            # Ø§Ú¯Ø± Ø§Ù„Ú¯ÙˆÛŒ Ø®Ø§ØµÛŒ Ø¯Ø§Ø±Ù‡ØŒ Ø³Ø¨Ú© Ø±Ùˆ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¨Ø¯Ù‡
            if conversation_patterns:
                pattern = conversation_patterns[0]
                style = pattern.get('response_style', '')
                if 'Ø¯ÙˆØ³ØªØ§Ù†Ù‡' in style and 'ğŸ˜Š' not in enhanced_response:
                    enhanced_response += " ğŸ˜Š"
                elif 'Ú¯Ø±Ù…' in style and 'ğŸ¦Š' not in enhanced_response:
                    enhanced_response += " ğŸ¦Š"
            
            return enhanced_response
        
        print("ğŸ“Š dataset Ù…ÙÛŒØ¯ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ù¾Ø§Ø³Ø® Ø§ÙˆÙ„ÛŒÙ‡ Ø­ÙØ¸ Ù…ÛŒâ€ŒØ´ÙˆØ¯")
        return initial_response
    
    def _structure_final_response(self, message: str, enhanced_response: str, analysis: Dict, web_info: Dict = None, code_analysis: Dict = None) -> str:
        """Ø³Ø§Ø®ØªØ§Ø±Ø¯Ù‡ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ù¾Ø§Ø³Ø®"""
        
        # Ø§Ú¯Ø± Ú©Ø¯ Ø¯Ø§Ø´ØªØŒ Ø³Ø§Ø®ØªØ§Ø± ØªØ®ØµØµÛŒ
        if code_analysis:
            return self._structure_code_response(enhanced_response, code_analysis)
        
        # Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙˆØ¨ Ø¯Ø§Ø´ØªØŒ Ø³Ø§Ø®ØªØ§Ø± Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ
        if web_info and web_info.get('summary'):
            return self._structure_web_response(enhanced_response, web_info)
        
        # Ø§Ú¯Ø± Ø³Ø¤Ø§Ù„ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø¨ÙˆØ¯ØŒ Ø³Ø§Ø®ØªØ§Ø± ØªÙØµÛŒÙ„ÛŒ
        if analysis.get('complexity') == 'complex':
            return self._structure_complex_response(enhanced_response, analysis)
        
        # Ø³Ø§Ø®ØªØ§Ø± Ø¹Ø§Ø¯ÛŒ
        return enhanced_response
    
    def _structure_code_response(self, response: str, code_analysis: Dict) -> str:
        """Ø³Ø§Ø®ØªØ§Ø±Ø¯Ù‡ÛŒ Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§ÛŒ Ú©Ø¯"""
        analysis = code_analysis['analysis']
        
        structured = f"{response}\n\n"
        
        if analysis['issues']:
            structured += "ğŸš¨ Ù…Ø´Ú©Ù„Ø§Øª:\n"
            for issue in analysis['issues'][:3]:
                structured += f"â€¢ Ø®Ø· {issue['line']}: {issue['message']}\n"
            structured += "\n"
        
        if analysis['suggestions']:
            structured += "ğŸ’¡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª:\n"
            for suggestion in analysis['suggestions'][:3]:
                structured += f"â€¢ {suggestion['message']}\n"
        
        return structured.strip()
    
    def _structure_web_response(self, response: str, web_info: Dict) -> str:
        """Ø³Ø§Ø®ØªØ§Ø±Ø¯Ù‡ÛŒ Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙˆØ¨"""
        structured = f"{response}\n\n"
        
        if web_info.get('sources'):
            structured += f"ğŸ“Š Ù…Ù†Ø§Ø¨Ø¹: {web_info['sources']} Ù†ØªÛŒØ¬Ù‡ Ø§Ø² Ø§ÛŒÙ†ØªØ±Ù†Øª"
        
        return structured.strip()
    
    def _structure_complex_response(self, response: str, analysis: Dict) -> str:
        """Ø³Ø§Ø®ØªØ§Ø±Ø¯Ù‡ÛŒ Ù¾Ø§Ø³Ø® Ø¨Ø±Ø§ÛŒ Ø³Ø¤Ø§Ù„Ø§Øª Ù¾ÛŒÚ†ÛŒØ¯Ù‡"""
        # Ø¨Ø±Ø§ÛŒ Ø³Ø¤Ø§Ù„Ø§Øª Ù¾ÛŒÚ†ÛŒØ¯Ù‡ØŒ Ù¾Ø§Ø³Ø® Ø±Ùˆ Ø¨Ù‡ØªØ± Ø³Ø§Ø®ØªØ§Ø± Ø¨Ø¯Ù‡
        lines = response.split('.')
        if len(lines) > 2:
            # Ø§ÙˆÙ„ÛŒÙ† Ø¬Ù…Ù„Ù‡ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø®Ù„Ø§ØµÙ‡
            summary = lines[0].strip() + "."
            # Ø¨Ù‚ÛŒÙ‡ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø¬Ø²Ø¦ÛŒØ§Øª
            details = '. '.join(lines[1:]).strip()
            
            return f"{summary}\n\nğŸ“ Ø¬Ø²Ø¦ÛŒØ§Øª: {details}"
        
        return response
    
    def _create_learning_prompt(self, message: str, response: str, analysis: Dict, context: List[Dict] = None) -> str:
        """ØªØ¨Ø¯ÛŒÙ„ Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¨Ù‡ prompt Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø¬Ø¯Ø¯"""
        
        # Ø³Ø§Ø®Øª prompt Ø³Ø§Ø®ØªØ§Ø±ÛŒØ§ÙØªÙ‡
        learning_prompt = f"""# Ù…Ú©Ø§Ù„Ù…Ù‡ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø±ÙˆØ¨Ø§Ù‡

## ØªØ­Ù„ÛŒÙ„ Ù¾ÛŒØ§Ù… Ú©Ø§Ø±Ø¨Ø±:
- Ø§Ø­Ø³Ø§Ø³: {analysis.get('emotion', 'Ù†Ø§Ù…Ø´Ø®Øµ')}
- Ù…ÙˆØ¶ÙˆØ¹: {analysis.get('topic', 'Ø¹Ù…ÙˆÙ…ÛŒ')}
- Ù‡Ø¯Ù: {analysis.get('intent', 'Ù…Ú©Ø§Ù„Ù…Ù‡')}
- Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ: {analysis.get('complexity', 'Ø³Ø§Ø¯Ù‡')}

## Context Ù‚Ø¨Ù„ÛŒ:
{self._format_context_for_learning(context)}

## Ù…Ú©Ø§Ù„Ù…Ù‡:
Ú©Ø§Ø±Ø¨Ø±: {message}
Ø±ÙˆØ¨Ø§Ù‡: {response}

## Ø§Ù„Ú¯ÙˆÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ:
Ø§ÛŒÙ† Ù…Ú©Ø§Ù„Ù…Ù‡ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø§Ø¨Ù‡ØŒ Ù¾Ø§Ø³Ø® Ù…Ù†Ø§Ø³Ø¨ Ø´Ø§Ù…Ù„:
- Ø³Ø¨Ú©: {self._extract_response_style(response)}
- Ø·ÙˆÙ„: {len(response.split())} Ú©Ù„Ù…Ù‡
- Ø³Ø§Ø®ØªØ§Ø±: {self._analyze_response_structure(response)}

## Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¢ÛŒÙ†Ø¯Ù‡:
Ø§Ú¯Ø± Ú©Ø§Ø±Ø¨Ø± Ù¾ÛŒØ§Ù… Ù…Ø´Ø§Ø¨Ù‡ÛŒ Ø¨Ø§ Ù‡Ù…ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ÙØ±Ø³ØªØ§Ø¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§Ø² Ø§ÛŒÙ† Ø§Ù„Ú¯Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯.
"""
        
        return learning_prompt
    
    def _format_context_for_learning(self, context: List[Dict] = None) -> str:
        """ÙØ±Ù…Øª Ú©Ø±Ø¯Ù† context Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ"""
        if not context:
            return "Ù‡ÛŒÚ† context Ù‚Ø¨Ù„ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª"
        
        formatted = ""
        for item in context[-3:]:  # Ø¢Ø®Ø±ÛŒÙ† 3 Ù…ÙˆØ±Ø¯
            content = item.get('content', '')[:100]
            formatted += f"- {content}\n"
        
        return formatted.strip()
    
    def _extract_response_style(self, response: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø³Ø¨Ú© Ù¾Ø§Ø³Ø®"""
        if "ğŸ˜Š" in response or "ğŸ¦Š" in response:
            return "Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ùˆ Ø´Ø§Ø¯"
        elif "ğŸ¤”" in response or "ğŸ’­" in response:
            return "ØªÙÚ©Ø±ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„ÛŒ"
        elif "âš ï¸" in response or "âŒ" in response:
            return "Ù‡Ø´Ø¯Ø§Ø±Ø¯Ù‡Ù†Ø¯Ù‡"
        elif "âœ…" in response or "ğŸ‘" in response:
            return "Ù…Ø«Ø¨Øª Ùˆ ØªØ£ÛŒÛŒØ¯ÛŒ"
        else:
            return "Ø¹Ø§Ø¯ÛŒ Ùˆ Ø®Ù†Ø«ÛŒ"
    
    def _analyze_response_structure(self, response: str) -> str:
        """ØªØ­Ù„ÛŒÙ„ Ø³Ø§Ø®ØªØ§Ø± Ù¾Ø§Ø³Ø®"""
        lines = response.split('\n')
        sentences = response.split('.')
        
        if len(lines) > 3:
            return "Ú†Ù†Ø¯Ø®Ø·ÛŒ Ùˆ Ø³Ø§Ø®ØªØ§Ø±ÛŒØ§ÙØªÙ‡"
        elif len(sentences) > 3:
            return "Ú†Ù†Ø¯Ø¬Ù…Ù„Ù‡â€ŒØ§ÛŒ Ùˆ ØªÙØµÛŒÙ„ÛŒ"
        elif '?' in response:
            return "ØªØ¹Ø§Ù…Ù„ÛŒ Ùˆ Ø³Ø¤Ø§Ù„â€ŒÙ…Ø­ÙˆØ±"
        else:
            return "Ø³Ø§Ø¯Ù‡ Ùˆ Ù…Ø³ØªÙ‚ÛŒÙ…"
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® fallback ÙˆÙ‚ØªÛŒ Ù…Ø¯Ù„ Ú©Ø§Ø± Ù†Ù…ÛŒâ€ŒÚ©Ù†Ø¯"""
        
        # Ø§Ú¯Ø± Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙˆØ¨ Ø¯Ø§Ø±ÛŒÙ…ØŒ Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§ Ø§ÙˆÙ† Ø¨Ø§Ø´Ù‡
        if web_info and web_info.get('summary'):
            return f"Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§ÛŒÙ†ØªØ±Ù†Øª:\n\n{web_info['summary']}\n\n(Ù…Ø¯Ù„ AI Ù…Ù† Ø§Ù„Ø§Ù† Ú©Ù…ÛŒ Ú©Ù†Ø¯ Ù‡Ø³ØªØŒ Ø§Ù…Ø§ Ø§ÛŒÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ùˆ Ø§Ø² Ø§ÛŒÙ†ØªØ±Ù†Øª Ø¨Ø±Ø§Øª Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù…! ğŸ¦Š)"
        
        # Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ fallback Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ø³Ø¤Ø§Ù„
        message_lower = message.lower()
        
        import random
        
        # Ø³Ø¤Ø§Ù„Ø§Øª Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§
        if any(word in message_lower for word in ["Ø¯Ù…Ø§", "Ù‡ÙˆØ§", "Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§", "Ø¨Ø§Ø±Ø´", "Ø¨Ø§Ø±Ø§Ù†", "Ø¨Ø±Ù", "Ú¯Ø±Ù…Ø§", "Ø³Ø±Ù…Ø§"]):
            responses = [
                "Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø§Ù„Ø§Ù† Ù†Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ù‚ÛŒÙ‚ Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§ Ø±Ùˆ Ø¨Ù‡Øª Ø¨Ø¯Ù…. Ø¨Ù‡ØªØ±Ù‡ Ø§Ø² Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ§Ø´Ù†Ø§Ø³ÛŒ Ú†Ú© Ú©Ù†ÛŒ! ğŸŒ¤ï¸",
                "Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ù‚ÛŒÙ‚ Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§ØŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ù… Ø§Ø² Ø§Ù¾ Ù‡ÙˆØ§Ø´Ù†Ø§Ø³ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒ ğŸŒ¡ï¸",
                "Ø§Ù„Ø§Ù† Ù…Ø´Ú©Ù„ ÙÙ†ÛŒ Ø¯Ø§Ø±Ù… Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§. Ø³Ø§ÛŒØª Ù‡ÙˆØ§Ø´Ù†Ø§Ø³ÛŒ Ø±Ùˆ Ú†Ú© Ú©Ù†! â˜ï¸"
            ]
            return random.choice(responses)
        
        # Ø³Ø¤Ø§Ù„Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ
        elif "ØŸ" in message:
            responses = [
                "Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø§Ù„Ø§Ù† Ù…Ø´Ú©Ù„ ÙÙ†ÛŒ Ø¯Ø§Ø±Ù… Ùˆ Ù†Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ø¬ÙˆØ§Ø¨ Ú©Ø§Ù…Ù„ÛŒ Ø¨Ø¯Ù…. Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†! ï¿½",
                "Ø¨Ø¨Ø®Ø´ÛŒØ¯ØŒ Ø§Ù„Ø§Ù† Ú©Ù…ÛŒ Ú©Ù†Ø¯Ù…! Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ Ø³Ø¤Ø§Ù„Øª Ø±Ùˆ Ø³Ø§Ø¯Ù‡â€ŒØªØ± Ø¨Ù¾Ø±Ø³ÛŒØŸ ğŸ˜…",
                "Ù…Ø¯Ù„ AI Ù…Ù† Ø§Ù„Ø§Ù† Ù…Ø´Ú©Ù„ Ø¯Ø§Ø±Ù‡. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†! ï¿½"
            ]
            return random.choice(responses)
        
        # Ø³Ù„Ø§Ù… Ùˆ Ø§Ø­ÙˆØ§Ù„â€ŒÙ¾Ø±Ø³ÛŒ
        elif any(word in message_lower for word in ["Ø³Ù„Ø§Ù…", "Ø¯Ø±ÙˆØ¯", "Ú†Ø·ÙˆØ±", "Ø­Ø§Ù„"]):
            responses = [
                "Ø³Ù„Ø§Ù…! Ø®ÙˆØ´Ø­Ø§Ù„Ù… Ú©Ù‡ Ø¨Ø§Ù‡Ø§Ù… Ø­Ø±Ù Ù…ÛŒâ€ŒØ²Ù†ÛŒ! ğŸ¦Š",
                "Ø¯Ø±ÙˆØ¯ Ø¨Ø± ØªÙˆ! Ú†Ø·ÙˆØ±ÛŒØŸ ï¿½",
                "Ø³Ù„Ø§Ù… Ø¹Ø²ÛŒØ²! Ø­Ø§Ù„Ù… Ø®ÙˆØ¨Ù‡ØŒ ØªÙˆ Ú†Ø·ÙˆØ±ÛŒØŸ ğŸŒŸ"
            ]
            return random.choice(responses)
        
        # Ù¾Ø§Ø³Ø® Ø¹Ù…ÙˆÙ…ÛŒ
        else:
            responses = [
                "Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø§Ù„Ø§Ù† Ù…Ø´Ú©Ù„ ÙÙ†ÛŒ Ø¯Ø§Ø±Ù…. Ù„Ø·ÙØ§Ù‹ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ„Ø§Ø´ Ú©Ù†! ï¿½",
                "Ø¨Ø¨Ø®Ø´ÛŒØ¯ØŒ Ù…Ø¯Ù„ AI Ù…Ù† Ú©Ù…ÛŒ Ú©Ù†Ø¯Ù‡. Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†! ğŸ˜…",
                "Ø§Ù„Ø§Ù† Ù…Ø´Ú©Ù„ Ø¯Ø§Ø±Ù…ØŒ Ø§Ù…Ø§ Ø®ÙˆØ´Ø­Ø§Ù„Ù… Ú©Ù‡ Ø¨Ø§Ù‡Ø§Ù… Ø­Ø±Ù Ù…ÛŒâ€ŒØ²Ù†ÛŒ! ğŸ’™"
            ]
            return random.choice(responses)
    def _is_topic_change_request(self, message: str) -> bool:
        """ØªØ´Ø®ÛŒØµ Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØªØºÛŒÛŒØ± Ù…ÙˆØ¶ÙˆØ¹"""
        message_lower = message.lower()
        
        # Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ ØªØºÛŒÛŒØ± Ù…ÙˆØ¶ÙˆØ¹
        topic_change_keywords = [
            "Ø¨ÛŒâ€ŒØ®ÛŒØ§Ù„", "Ø¨ÛŒØ®ÛŒØ§Ù„", "ÙˆÙ„Ø´ Ú©Ù†", "ÙˆÙ„Ø´", "ÙØ±Ø§Ù…ÙˆØ´ Ú©Ù†", "ÙØ±Ø§Ù…ÙˆØ´Ø´ Ú©Ù†",
            "Ù…ÙˆØ¶ÙˆØ¹ Ø¬Ø¯ÛŒØ¯", "Ù…ÙˆØ¶ÙˆØ¹ ØªØ§Ø²Ù‡", "Ú†ÛŒØ² Ø¬Ø¯ÛŒØ¯", "Ú†ÛŒØ² ØªØ§Ø²Ù‡", "Ø¨Ø­Ø« Ø¬Ø¯ÛŒØ¯",
            "Ø´Ø±ÙˆØ¹ Ú©Ù†ÛŒÙ…", "Ø´Ø±ÙˆØ¹ Ú©Ù†", "Ø¨ÛŒØ§ Ø´Ø±ÙˆØ¹", "Ø§Ø² Ù†Ùˆ Ø´Ø±ÙˆØ¹", "ØªØ§Ø²Ù‡ Ø´Ø±ÙˆØ¹",
            "Ø¹ÙˆØ¶ Ú©Ù†", "ØªØºÛŒÛŒØ± Ø¨Ø¯Ù‡", "Ø¨Ø°Ø§Ø± Ø¨Ø±ÙˆÛŒÙ…", "Ø¨Ø±ÛŒÙ… Ø³Ø±Ø§Øº", "Ø­Ø§Ù„Ø§ Ø¨ÛŒØ§",
            "Ø¯ÛŒÚ¯Ù‡ Ù†Ù…ÛŒâ€ŒØ®ÙˆØ§Ù…", "Ø¯ÛŒÚ¯Ù‡ Ù†Ù…ÛŒØ®ÙˆØ§Ù…", "Ú©Ø§ÙÛŒÙ‡", "Ø¨Ø³Ù‡", "ØªÙ…Ø§Ù…",
            "ÛŒÙ‡ Ú†ÛŒØ² Ø¯ÛŒÚ¯Ù‡", "ÛŒÙ‡ Ú†ÛŒØ² Ø¯ÛŒÚ¯Ø±", "Ú†ÛŒØ² Ø¯ÛŒÚ¯Ù‡", "Ú†ÛŒØ² Ø¯ÛŒÚ¯Ø±",
            "Ù…ÙˆØ¶ÙˆØ¹ Ø¯ÛŒÚ¯Ù‡", "Ù…ÙˆØ¶ÙˆØ¹ Ø¯ÛŒÚ¯Ø±", "Ø¨Ø­Ø« Ø¯ÛŒÚ¯Ù‡", "Ø¨Ø­Ø« Ø¯ÛŒÚ¯Ø±"
        ]
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
        for keyword in topic_change_keywords:
            if keyword in message_lower:
                return True
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ø¬Ù…Ù„Ù‡
        change_patterns = [
            "Ø¨ÛŒ.*Ø®ÛŒØ§Ù„.*Ù…ÙˆØ¶ÙˆØ¹",
            "ÙˆÙ„Ø´.*Ú©Ù†.*Ø¨ÛŒØ§",
            "Ù…ÙˆØ¶ÙˆØ¹.*Ø¬Ø¯ÛŒØ¯.*Ø´Ø±ÙˆØ¹",
            "Ø´Ø±ÙˆØ¹.*Ú©Ù†ÛŒÙ….*Ú†ÛŒØ²",
            "Ø¨Ø±ÛŒÙ….*Ø³Ø±Ø§Øº.*Ú†ÛŒØ²",
            "Ø­Ø§Ù„Ø§.*Ø¨ÛŒØ§.*Ø¯Ø±Ø¨Ø§Ø±Ù‡"
        ]
        
        import re
        for pattern in change_patterns:
            if re.search(pattern, message_lower):
                return True
        
        return False
    def _check_topic_continuity(self, current_topic: str, context: List[Dict] = None) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¯Ø§Ù…Ù‡ Ù…ÙˆØ¶ÙˆØ¹ Ù‚Ø¨Ù„ÛŒ"""
        if not context or len(context) < 2:
            return False
        
        if not self.current_conversation_topic:
            return False
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª ØªØºÛŒÛŒØ± Ù…ÙˆØ¶ÙˆØ¹ Ø¯Ø± Ù¾ÛŒØ§Ù… ÙØ¹Ù„ÛŒ
        current_message = context[-1].get('content', '') if context else ""
        if self._is_topic_change_request(current_message):
            print("ğŸ”„ ØªØºÛŒÛŒØ± Ù…ÙˆØ¶ÙˆØ¹ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø´Ø¯Ù‡ - Ø§Ø¯Ø§Ù…Ù‡ Ù…ÙˆØ¶ÙˆØ¹ Ù‚Ø¨Ù„ÛŒ: Ø®ÛŒØ±")
            return False
        
        # Ø§Ú¯Ø± Ù…ÙˆØ¶ÙˆØ¹ ÙØ¹Ù„ÛŒ Ø¨Ø§ Ù…ÙˆØ¶ÙˆØ¹ Ù‚Ø¨Ù„ÛŒ ÛŒÚ©ÛŒ Ø¨Ø§Ø´Ù‡
        if current_topic == self.current_conversation_topic:
            return True
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù„Ù…Ø§Øª Ù…Ø±ØªØ¨Ø· Ø¯Ø± Ù¾ÛŒØ§Ù… ÙØ¹Ù„ÛŒ (Ù†Ù‡ Ú©Ù„ context)
        
        # Ø§Ú¯Ø± Ú©Ù„Ù…Ø§Øª Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ù…ÙˆØ¶ÙˆØ¹ Ù‚Ø¨Ù„ÛŒ Ø¯Ø± Ù¾ÛŒØ§Ù… ÙØ¹Ù„ÛŒ Ø¨Ø§Ø´Ù‡
        topic_keywords = self._get_topic_keywords(self.current_conversation_topic)
        keyword_matches = sum(1 for keyword in topic_keywords if keyword in current_message.lower())
        
        # Ø­Ø¯Ø§Ù‚Ù„ 1 Ú©Ù„Ù…Ù‡ Ù…Ø±ØªØ¨Ø· Ú©Ø§ÙÛŒÙ‡ (Ù†Ù‡ 2)
        return keyword_matches >= 1
    
    def _get_topic_keywords(self, topic: str) -> List[str]:
        """Ø¯Ø±ÛŒØ§ÙØª Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù‡Ø± Ù…ÙˆØ¶ÙˆØ¹ - Ø§Ø² Ù…ÙˆØ¶ÙˆØ¹Ø§Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø´Ø¯Ù‡ Ùˆ Ù¾Ø§ÛŒÙ‡"""
        
        # Ø§Ø¨ØªØ¯Ø§ Ø§Ø² Ù…ÙˆØ¶ÙˆØ¹Ø§Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø´Ø¯Ù‡ Ø¨Ú¯ÛŒØ±
        learned_topics = self._load_learned_topics()
        if topic in learned_topics:
            return learned_topics[topic]['keywords']
        
        # Ø§Ú¯Ø± Ø¯Ø± Ù…ÙˆØ¶ÙˆØ¹Ø§Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø´Ø¯Ù‡ Ù†Ø¨ÙˆØ¯ØŒ Ø§Ø² Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù¾Ø§ÛŒÙ‡ Ø¨Ú¯ÛŒØ±
        static_topic_keywords = {
            "Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ": ["Ú©Ø¯", "Ø¨Ø±Ù†Ø§Ù…Ù‡", "python", "javascript", "html", "css", "function", "variable", "loop", "ØªØ§Ø¨Ø¹", "Ù…ØªØºÛŒØ±"],
            "Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§": ["Ø¯Ù…Ø§", "Ù‡ÙˆØ§", "Ø¨Ø§Ø±Ø´", "Ø¨Ø§Ø±Ø§Ù†", "Ø¨Ø±Ù", "Ø¢ÙØªØ§Ø¨ÛŒ", "Ø§Ø¨Ø±ÛŒ", "Ú¯Ø±Ù…Ø§", "Ø³Ø±Ù…Ø§", "Ù‡ÙˆØ§Ø´Ù†Ø§Ø³ÛŒ"],
            "Ø¢Ø´Ù¾Ø²ÛŒ": ["ØºØ°Ø§", "Ù¾Ø®Øª", "Ø¯Ø³ØªÙˆØ±", "Ù…ÙˆØ§Ø¯", "Ø·Ø¨Ø®", "Ø¢Ø´Ù¾Ø²ÛŒ", "Ø®ÙˆØ±Ø§Ú©", "Ø·Ø¹Ø§Ù…"],
            "Ø³ÙØ±": ["Ø³ÙØ±", "Ù…Ø³Ø§ÙØ±Øª", "Ø´Ù‡Ø±", "Ú©Ø´ÙˆØ±", "Ù‡ØªÙ„", "Ø¨Ù„ÛŒØ·", "Ú¯Ø±Ø¯Ø´Ú¯Ø±ÛŒ", "Ø¬Ø§Ù‡Ø§ÛŒ Ø¯ÛŒØ¯Ù†ÛŒ"],
            "ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ": ["Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±", "Ù…ÙˆØ¨Ø§ÛŒÙ„", "Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù†", "Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø±", "Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ", "AI", "ÙÙ†Ø§ÙˆØ±ÛŒ"],
            "Ø³Ù„Ø§Ù…ØªÛŒ": ["Ø³Ù„Ø§Ù…Øª", "Ø¨ÛŒÙ…Ø§Ø±ÛŒ", "Ø¯Ú©ØªØ±", "Ø¯Ø§Ø±Ùˆ", "ÙˆØ±Ø²Ø´", "ØªØºØ°ÛŒÙ‡", "Ø¨Ù‡Ø¯Ø§Ø´Øª"],
            "ØªØ­ØµÛŒÙ„": ["Ø¯Ø±Ø³", "Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡", "Ù…Ø¯Ø±Ø³Ù‡", "Ø§Ù…ØªØ­Ø§Ù†", "ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ", "Ú©ØªØ§Ø¨", "Ù…Ø·Ø§Ù„Ø¹Ù‡"]
        }
        
        return static_topic_keywords.get(topic, [])
    def get_learned_topics_summary(self) -> Dict:
        """Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø´Ø¯Ù‡"""
        learned_topics = self._load_learned_topics()
        
        summary = {
            "total_topics": len(learned_topics),
            "topics": {}
        }
        
        for topic_name, topic_data in learned_topics.items():
            summary["topics"][topic_name] = {
                "keywords_count": len(topic_data.get('keywords', [])),
                "usage_count": topic_data.get('usage_count', 0),
                "created_at": topic_data.get('created_at', ''),
                "sample_keywords": topic_data.get('keywords', [])[:5]  # Ù†Ù…Ø§ÛŒØ´ 5 Ú©Ù„Ù…Ù‡ Ø§ÙˆÙ„
            }
        
        return summary
    
    def reset_learned_topics(self):
        """Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… Ù…ÙˆØ¶ÙˆØ¹Ø§Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø´Ø¯Ù‡"""
        topics_file = "data/learning/learned_topics.json"
        if os.path.exists(topics_file):
            os.remove(topics_file)
        print("ğŸ—‘ï¸ ØªÙ…Ø§Ù… Ù…ÙˆØ¶ÙˆØ¹Ø§Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø´Ø¯Ù‡ Ù¾Ø§Ú© Ø´Ø¯Ù†Ø¯")
