"""
ğŸ¤– Ø±Ø§Ø¨Ø· Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø± ÙÛŒØ²ÛŒÚ©ÛŒ Ø±ÙˆØ¨Ø§Ù‡
Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø±ÙˆÛŒ Ø±Ø¨Ø§Øª Ù…ØªØ­Ø±Ú©
"""

import asyncio
import json
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from enum import Enum
from dataclasses import dataclass
import math

class MovementType(Enum):
    WALK = "walk"
    TURN = "turn"
    STOP = "stop"
    GESTURE = "gesture"
    HEAD_MOVE = "head_move"
    APPROACH = "approach"
    RETREAT = "retreat"

class EmotionExpression(Enum):
    HAPPY = "happy"
    CURIOUS = "curious"
    THINKING = "thinking"
    CONCERNED = "concerned"
    EXCITED = "excited"
    CALM = "calm"
    FOCUSED = "focused"

class SensorType(Enum):
    CAMERA = "camera"
    MICROPHONE = "microphone"
    PROXIMITY = "proximity"
    TOUCH = "touch"
    TEMPERATURE = "temperature"
    LIGHT = "light"
    MOTION = "motion"

@dataclass
class PhysicalAction:
    """Ø¹Ù…Ù„ ÙÛŒØ²ÛŒÚ©ÛŒ"""
    action_type: MovementType
    parameters: Dict
    duration: float
    priority: int
    emotion: Optional[EmotionExpression] = None

@dataclass
class SensorData:
    """Ø¯Ø§Ø¯Ù‡ Ø­Ø³Ú¯Ø±"""
    sensor_type: SensorType
    value: any
    timestamp: datetime
    confidence: float

class PhysicalInterface:
    def __init__(self):
        self.is_physical_mode = False  # ÙØ¹Ù„Ø§Ù‹ Ø¯Ø± Ø­Ø§Ù„Øª Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ
        self.current_position = {"x": 0, "y": 0, "rotation": 0}
        self.current_emotion = EmotionExpression.CALM
        self.battery_level = 1.0
        self.is_moving = False
        
        # ØµÙ Ø§Ø¹Ù…Ø§Ù„ ÙÛŒØ²ÛŒÚ©ÛŒ
        self.action_queue = asyncio.Queue()
        self.current_action = None
        
        # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø³Ú¯Ø±Ù‡Ø§
        self.sensor_data = {}
        self.environmental_awareness = {
            "room_map": {},
            "known_objects": {},
            "owner_location": None,
            "obstacles": []
        }
        
        # Ø±ÙØªØ§Ø±Ù‡Ø§ÛŒ Ø´Ø®ØµÛŒØªÛŒ ÙÛŒØ²ÛŒÚ©ÛŒ
        self.physical_personality = {
            "movement_speed": 0.7,      # Ø³Ø±Ø¹Øª Ø­Ø±Ú©Øª (0-1)
            "gesture_frequency": 0.5,   # ØªÚ©Ø±Ø§Ø± Ø­Ø±Ú©Ø§Øª (0-1)
            "personal_space": 1.0,      # ÙØ§ØµÙ„Ù‡ Ø´Ø®ØµÛŒ (Ù…ØªØ±)
            "eye_contact_level": 0.8,   # Ø³Ø·Ø­ ØªÙ…Ø§Ø³ Ú†Ø´Ù…ÛŒ (0-1)
            "expressiveness": 0.7       # Ø¨ÛŒØ§Ù† Ø§Ø­Ø³Ø§Ø³Ø§Øª (0-1)
        }
        
        print("ğŸ¤– Ø±Ø§Ø¨Ø· Ø³Ø®Øªâ€ŒØ§ÙØ²Ø§Ø± ÙÛŒØ²ÛŒÚ©ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯ (Ø­Ø§Ù„Øª Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ)")
    
    async def express_emotion(self, emotion: EmotionExpression, intensity: float = 1.0):
        """Ø¨ÛŒØ§Ù† Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ø² Ø·Ø±ÛŒÙ‚ Ø­Ø±Ú©Ø§Øª ÙÛŒØ²ÛŒÚ©ÛŒ"""
        
        self.current_emotion = emotion
        
        # ØªØ¹Ø±ÛŒÙ Ø­Ø±Ú©Ø§Øª Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø§Ø­Ø³Ø§Ø³
        emotion_actions = {
            EmotionExpression.HAPPY: [
                PhysicalAction(MovementType.GESTURE, {"type": "wave"}, 2.0, 1),
                PhysicalAction(MovementType.HEAD_MOVE, {"direction": "nod"}, 1.0, 2)
            ],
            EmotionExpression.CURIOUS: [
                PhysicalAction(MovementType.HEAD_MOVE, {"direction": "tilt"}, 1.5, 1),
                PhysicalAction(MovementType.APPROACH, {"distance": 0.3}, 2.0, 2)
            ],
            EmotionExpression.THINKING: [
                PhysicalAction(MovementType.HEAD_MOVE, {"direction": "look_up"}, 2.0, 1),
                PhysicalAction(MovementType.GESTURE, {"type": "chin_touch"}, 3.0, 2)
            ],
            EmotionExpression.CONCERNED: [
                PhysicalAction(MovementType.HEAD_MOVE, {"direction": "shake"}, 1.0, 1),
                PhysicalAction(MovementType.APPROACH, {"distance": 0.5}, 1.5, 2)
            ]
        }
        
        if emotion in emotion_actions:
            for action in emotion_actions[emotion]:
                action.emotion = emotion
                await self.action_queue.put(action)
        
        print(f"ğŸ­ Ø¨ÛŒØ§Ù† Ø§Ø­Ø³Ø§Ø³: {emotion.value} Ø¨Ø§ Ø´Ø¯Øª {intensity}")
    
    async def move_to_owner(self, urgency: float = 0.5):
        """Ø­Ø±Ú©Øª Ø¨Ù‡ Ø³Ù…Øª Ù…Ø§Ù„Ú©"""
        
        if not self.environmental_awareness["owner_location"]:
            # Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø§Ù„Ú©
            await self.search_for_owner()
            return
        
        owner_pos = self.environmental_awareness["owner_location"]
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø³ÛŒØ±
        path = self._calculate_path_to_position(owner_pos)
        
        # ØªØ¹ÛŒÛŒÙ† ÙØ§ØµÙ„Ù‡ Ù…Ù†Ø§Ø³Ø¨ (Ø§Ø­ØªØ±Ø§Ù… Ø¨Ù‡ ÙØ¶Ø§ÛŒ Ø´Ø®ØµÛŒ)
        approach_distance = self.physical_personality["personal_space"]
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ø¹Ù…Ù„ Ø­Ø±Ú©Øª
        move_action = PhysicalAction(
            action_type=MovementType.APPROACH,
            parameters={
                "target": owner_pos,
                "stop_distance": approach_distance,
                "path": path,
                "speed": min(1.0, urgency + 0.3)
            },
            duration=self._estimate_movement_time(path),
            priority=int(urgency * 10),
            emotion=EmotionExpression.FOCUSED
        )
        
        await self.action_queue.put(move_action)
        print(f"ğŸš¶ Ø­Ø±Ú©Øª Ø¨Ù‡ Ø³Ù…Øª Ù…Ø§Ù„Ú© Ø¨Ø§ ÙÙˆØ±ÛŒØª {urgency}")
    
    async def search_for_owner(self):
        """Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø§Ù„Ú© Ø¯Ø± Ù…Ø­ÛŒØ·"""
        
        search_action = PhysicalAction(
            action_type=MovementType.TURN,
            parameters={
                "angle": 360,
                "speed": 0.3,
                "scan_mode": True
            },
            duration=10.0,
            priority=8,
            emotion=EmotionExpression.CURIOUS
        )
        
        await self.action_queue.put(search_action)
        print("ğŸ” Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…Ø§Ù„Ú©...")
    
    async def respond_to_call(self, call_location: Tuple[float, float]):
        """Ù¾Ø§Ø³Ø® Ø¨Ù‡ ØµØ¯Ø§ Ø²Ø¯Ù† Ù…Ø§Ù„Ú©"""
        
        # Ú†Ø±Ø®Ø´ Ø¨Ù‡ Ø³Ù…Øª ØµØ¯Ø§
        turn_angle = self._calculate_turn_angle(call_location)
        
        turn_action = PhysicalAction(
            action_type=MovementType.TURN,
            parameters={"angle": turn_angle, "speed": 0.8},
            duration=abs(turn_angle) / 90,  # Ø²Ù…Ø§Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ø²Ø§ÙˆÛŒÙ‡
            priority=9,
            emotion=EmotionExpression.EXCITED
        )
        
        await self.action_queue.put(turn_action)
        
        # Ø­Ø±Ú©Øª Ø¨Ù‡ Ø³Ù…Øª Ù…Ø§Ù„Ú©
        await self.move_to_owner(urgency=0.8)
        
        print(f"ğŸ“ Ù¾Ø§Ø³Ø® Ø¨Ù‡ ØµØ¯Ø§ÛŒ Ù…Ø§Ù„Ú© Ø§Ø² Ù…ÙˆÙ‚Ø¹ÛŒØª {call_location}")
    
    async def perform_task_gesture(self, task_type: str):
        """Ø§Ù†Ø¬Ø§Ù… Ø­Ø±Ú©Øª Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ù†ÙˆØ¹ Ú©Ø§Ø±"""
        
        task_gestures = {
            "presentation": [
                PhysicalAction(MovementType.GESTURE, {"type": "point"}, 2.0, 5),
                PhysicalAction(MovementType.HEAD_MOVE, {"direction": "look_at_screen"}, 1.0, 4)
            ],
            "explanation": [
                PhysicalAction(MovementType.GESTURE, {"type": "open_hands"}, 1.5, 5),
                PhysicalAction(MovementType.HEAD_MOVE, {"direction": "face_owner"}, 1.0, 6)
            ],
            "thinking": [
                PhysicalAction(MovementType.HEAD_MOVE, {"direction": "look_up"}, 2.0, 3),
                PhysicalAction(MovementType.GESTURE, {"type": "chin_touch"}, 3.0, 2)
            ],
            "agreement": [
                PhysicalAction(MovementType.HEAD_MOVE, {"direction": "nod"}, 1.0, 7),
                PhysicalAction(MovementType.GESTURE, {"type": "thumbs_up"}, 1.5, 6)
            ]
        }
        
        if task_type in task_gestures:
            for gesture in task_gestures[task_type]:
                await self.action_queue.put(gesture)
        
        print(f"ğŸ‘‹ Ø§Ù†Ø¬Ø§Ù… Ø­Ø±Ú©Øª Ø¨Ø±Ø§ÛŒ: {task_type}")
    
    async def maintain_attention(self):
        """Ø­ÙØ¸ ØªÙˆØ¬Ù‡ Ùˆ Ø­Ø¶ÙˆØ± ÙØ¹Ø§Ù„"""
        
        # Ø­Ø±Ú©Ø§Øª Ø¸Ø±ÛŒÙ Ø¨Ø±Ø§ÛŒ Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯Ù† Ø²Ù†Ø¯Ù‡ Ø¨ÙˆØ¯Ù†
        subtle_actions = [
            PhysicalAction(MovementType.HEAD_MOVE, {"direction": "slight_tilt"}, 0.5, 1),
            PhysicalAction(MovementType.GESTURE, {"type": "micro_adjustment"}, 0.3, 1)
        ]
        
        # Ø§Ù†ØªØ®Ø§Ø¨ ØªØµØ§Ø¯ÙÛŒ Ø­Ø±Ú©Øª Ø¸Ø±ÛŒÙ
        import random
        action = random.choice(subtle_actions)
        await self.action_queue.put(action)
    
    async def process_sensor_input(self, sensor_type: SensorType, data: any) -> Dict:
        """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙˆØ±ÙˆØ¯ÛŒ Ø­Ø³Ú¯Ø±Ù‡Ø§"""
        
        sensor_reading = SensorData(
            sensor_type=sensor_type,
            value=data,
            timestamp=datetime.now(),
            confidence=0.8
        )
        
        self.sensor_data[sensor_type.value] = sensor_reading
        
        # ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø³Ú¯Ø±
        analysis = await self._analyze_sensor_data(sensor_reading)
        
        # ÙˆØ§Ú©Ù†Ø´ Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…
        if analysis.get("requires_action", False):
            await self._react_to_sensor_data(analysis)
        
        return analysis
    
    async def _analyze_sensor_data(self, sensor_data: SensorData) -> Dict:
        """ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø³Ú¯Ø±"""
        
        analysis = {"requires_action": False}
        
        if sensor_data.sensor_type == SensorType.PROXIMITY:
            distance = sensor_data.value
            if distance < 0.5:  # Ø®ÛŒÙ„ÛŒ Ù†Ø²Ø¯ÛŒÚ©
                analysis.update({
                    "requires_action": True,
                    "action_type": "retreat",
                    "reason": "too_close"
                })
            elif distance > 3.0:  # Ø®ÛŒÙ„ÛŒ Ø¯ÙˆØ±
                analysis.update({
                    "requires_action": True,
                    "action_type": "approach",
                    "reason": "too_far"
                })
        
        elif sensor_data.sensor_type == SensorType.MOTION:
            if sensor_data.value:  # Ø­Ø±Ú©Øª ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯
                analysis.update({
                    "requires_action": True,
                    "action_type": "attention",
                    "reason": "motion_detected"
                })
        
        return analysis
    
    async def _react_to_sensor_data(self, analysis: Dict):
        """ÙˆØ§Ú©Ù†Ø´ Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ø³Ú¯Ø±"""
        
        action_type = analysis.get("action_type")
        
        if action_type == "retreat":
            retreat_action = PhysicalAction(
                action_type=MovementType.RETREAT,
                parameters={"distance": 0.5},
                duration=2.0,
                priority=7,
                emotion=EmotionExpression.CONCERNED
            )
            await self.action_queue.put(retreat_action)
        
        elif action_type == "approach":
            approach_action = PhysicalAction(
                action_type=MovementType.APPROACH,
                parameters={"distance": 0.3},
                duration=3.0,
                priority=5,
                emotion=EmotionExpression.CURIOUS
            )
            await self.action_queue.put(approach_action)
        
        elif action_type == "attention":
            await self.express_emotion(EmotionExpression.CURIOUS, 0.7)
    
    def _calculate_path_to_position(self, target_pos: Tuple[float, float]) -> List[Tuple[float, float]]:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø³ÛŒØ± Ø¨Ù‡ Ù…ÙˆÙ‚Ø¹ÛŒØª Ù‡Ø¯Ù"""
        
        current_pos = (self.current_position["x"], self.current_position["y"])
        
        # Ù…Ø³ÛŒØ± Ø³Ø§Ø¯Ù‡ (Ø®Ø· Ù…Ø³ØªÙ‚ÛŒÙ…) - Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ± Ø´ÙˆØ¯
        path = [current_pos, target_pos]
        
        # Ø¯Ø± Ø¢ÛŒÙ†Ø¯Ù‡: obstacle avoidance, path optimization
        
        return path
    
    def _estimate_movement_time(self, path: List[Tuple[float, float]]) -> float:
        """ØªØ®Ù…ÛŒÙ† Ø²Ù…Ø§Ù† Ø­Ø±Ú©Øª"""
        
        total_distance = 0
        for i in range(len(path) - 1):
            dx = path[i+1][0] - path[i][0]
            dy = path[i+1][1] - path[i][1]
            distance = math.sqrt(dx*dx + dy*dy)
            total_distance += distance
        
        speed = self.physical_personality["movement_speed"]
        return total_distance / (speed * 0.5)  # 0.5 m/s base speed
    
    def _calculate_turn_angle(self, target_pos: Tuple[float, float]) -> float:
        """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø²Ø§ÙˆÛŒÙ‡ Ú†Ø±Ø®Ø´"""
        
        current_pos = (self.current_position["x"], self.current_position["y"])
        current_rotation = self.current_position["rotation"]
        
        dx = target_pos[0] - current_pos[0]
        dy = target_pos[1] - current_pos[1]
        
        target_angle = math.degrees(math.atan2(dy, dx))
        turn_angle = target_angle - current_rotation
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø²Ø§ÙˆÛŒÙ‡
        while turn_angle > 180:
            turn_angle -= 360
        while turn_angle < -180:
            turn_angle += 360
        
        return turn_angle
    
    async def start_physical_loop(self):
        """Ø´Ø±ÙˆØ¹ Ø­Ù„Ù‚Ù‡ Ø§ØµÙ„ÛŒ ÙÛŒØ²ÛŒÚ©ÛŒ"""
        
        print("ğŸ”„ Ø´Ø±ÙˆØ¹ Ø­Ù„Ù‚Ù‡ ÙÛŒØ²ÛŒÚ©ÛŒ Ø±ÙˆØ¨Ø§Ù‡")
        
        while True:
            try:
                # Ù¾Ø±Ø¯Ø§Ø²Ø´ ØµÙ Ø§Ø¹Ù…Ø§Ù„
                if not self.action_queue.empty():
                    action = await self.action_queue.get()
                    await self._execute_physical_action(action)
                
                # Ø­ÙØ¸ ØªÙˆØ¬Ù‡ (Ù‡Ø± 30 Ø«Ø§Ù†ÛŒÙ‡)
                if not self.is_moving:
                    await asyncio.sleep(30)
                    await self.maintain_attention()
                
                await asyncio.sleep(0.1)  # 10 FPS
                
            except Exception as e:
                print(f"Ø®Ø·Ø§ Ø¯Ø± Ø­Ù„Ù‚Ù‡ ÙÛŒØ²ÛŒÚ©ÛŒ: {e}")
                await asyncio.sleep(1)
    
    async def _execute_physical_action(self, action: PhysicalAction):
        """Ø§Ø¬Ø±Ø§ÛŒ Ø¹Ù…Ù„ ÙÛŒØ²ÛŒÚ©ÛŒ"""
        
        self.current_action = action
        self.is_moving = True
        
        print(f"ğŸ¬ Ø§Ø¬Ø±Ø§ÛŒ Ø¹Ù…Ù„: {action.action_type.value} - {action.parameters}")
        
        # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø¬Ø±Ø§ÛŒ Ø¹Ù…Ù„
        await asyncio.sleep(action.duration)
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ ÙˆØ¶Ø¹ÛŒØª
        if action.action_type == MovementType.TURN:
            self.current_position["rotation"] += action.parameters.get("angle", 0)
        elif action.action_type in [MovementType.APPROACH, MovementType.WALK]:
            # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÙˆÙ‚Ø¹ÛŒØª (Ø³Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡)
            pass
        
        self.is_moving = False
        self.current_action = None
        
        print(f"âœ… Ø¹Ù…Ù„ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯: {action.action_type.value}")
    
    def get_physical_status(self) -> Dict:
        """ÙˆØ¶Ø¹ÛŒØª ÙÛŒØ²ÛŒÚ©ÛŒ ÙØ¹Ù„ÛŒ"""
        
        return {
            "position": self.current_position,
            "emotion": self.current_emotion.value,
            "battery_level": self.battery_level,
            "is_moving": self.is_moving,
            "current_action": self.current_action.action_type.value if self.current_action else None,
            "queue_size": self.action_queue.qsize(),
            "sensor_count": len(self.sensor_data),
            "physical_mode": self.is_physical_mode
        }
    
    def enable_physical_mode(self):
        """ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø­Ø§Ù„Øª ÙÛŒØ²ÛŒÚ©ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ"""
        self.is_physical_mode = True
        print("ğŸ¤– Ø­Ø§Ù„Øª ÙÛŒØ²ÛŒÚ©ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ ÙØ¹Ø§Ù„ Ø´Ø¯!")
    
    def disable_physical_mode(self):
        """ØºÛŒØ±ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø­Ø§Ù„Øª ÙÛŒØ²ÛŒÚ©ÛŒ"""
        self.is_physical_mode = False
        print("ğŸ’» Ø¨Ø§Ø²Ú¯Ø´Øª Ø¨Ù‡ Ø­Ø§Ù„Øª Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ")

# Instance Ø³Ø±Ø§Ø³Ø±ÛŒ
physical_interface = PhysicalInterface()