"""
Ø³ÛŒØ³ØªÙ… Ø­Ø§ÙØ¸Ù‡ Ø±ÙˆØ¨Ø§Ù‡
Ù…Ø¯ÛŒØ±ÛŒØª Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
"""

import chromadb
from chromadb.config import Settings
import json
import os
from datetime import datetime
from typing import List, Dict, Optional
import hashlib

class MemoryManager:
    def __init__(self):
        # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ ChromaDB Ø¨Ø±Ø§ÛŒ Ø­Ø§ÙØ¸Ù‡ vector
        self.chroma_client = chromadb.PersistentClient(
            path="data/memory",
            settings=Settings(anonymized_telemetry=False)
        )
        
        # Collection Ø¨Ø±Ø§ÛŒ Ù…Ú©Ø§Ù„Ù…Ø§Øª
        self.conversations = self.chroma_client.get_or_create_collection(
            name="conversations",
            metadata={"description": "ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…Ú©Ø§Ù„Ù…Ø§Øª Ú©Ø§Ø±Ø¨Ø±"}
        )
        
        # Collection Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ø´ Ú©Ù„ÛŒ
        self.knowledge = self.chroma_client.get_or_create_collection(
            name="knowledge", 
            metadata={"description": "Ø¯Ø§Ù†Ø´ Ùˆ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø´Ø¯Ù‡"}
        )
        
        # Ø­Ø§ÙØ¸Ù‡ Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª (Ø¯Ø± RAM)
        self.short_term_memory = []
        self.max_short_term = 50
        
        print("ðŸ§  Ø³ÛŒØ³ØªÙ… Ø­Ø§ÙØ¸Ù‡ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
    
    def store_conversation(self, role: str, content: str, metadata: Dict = None):
        """Ø°Ø®ÛŒØ±Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡"""
        
        timestamp = datetime.now()
        conversation_id = self._generate_id(f"{role}_{content}_{timestamp}")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡ Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª
        memory_item = {
            "id": conversation_id,
            "role": role,
            "content": content,
            "timestamp": timestamp.isoformat(),
            "metadata": metadata or {}
        }
        
        self.short_term_memory.append(memory_item)
        
        # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø­Ø§ÙØ¸Ù‡ Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª
        if len(self.short_term_memory) > self.max_short_term:
            # Ø§Ù†ØªÙ‚Ø§Ù„ Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ±ÛŒÙ† Ø¢ÛŒØªÙ… Ø¨Ù‡ Ø­Ø§ÙØ¸Ù‡ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª
            old_item = self.short_term_memory.pop(0)
            self._store_to_long_term(old_item)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ù…Ú©Ø§Ù„Ù…Ø§Øª Ø¯Ø± vector database
        if self._is_important_conversation(content):
            self._store_to_vector_db(memory_item)
        
        print(f"ðŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {role} - {content[:50]}...")
    
    def get_relevant_context(self, query: str, limit: int = 5) -> List[Dict]:
        """Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ context Ù…Ø±ØªØ¨Ø· Ø¨Ø±Ø§ÛŒ query"""
        
        relevant_memories = []
        
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡ Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª
        for item in reversed(self.short_term_memory[-10:]):  # Ø¢Ø®Ø±ÛŒÙ† 10 Ù…ÙˆØ±Ø¯
            relevant_memories.append(item)
        
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± vector database Ø¨Ø§ error handling
        try:
            results = self.conversations.query(
                query_texts=[query],
                n_results=min(limit, 10)
            )
            
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    metadata = results['metadatas'][0][i] if results['metadatas'] else {}
                    relevant_memories.append({
                        "content": doc,
                        "metadata": metadata,
                        "similarity_score": results['distances'][0][i] if results['distances'] else 0
                    })
        
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ vector: {e}")
            # Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§ØŒ ÙÙ‚Ø· Ø§Ø² Ø­Ø§ÙØ¸Ù‡ Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
            print("ðŸ”„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø­Ø§ÙØ¸Ù‡ Ú©ÙˆØªØ§Ù‡â€ŒÙ…Ø¯Øª Ø¨Ù‡ Ø¬Ø§ÛŒ vector search")
        
        return relevant_memories[:limit]
    
    def get_memory_count(self) -> Dict[str, int]:
        """ØªØ¹Ø¯Ø§Ø¯ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø­Ø§ÙØ¸Ù‡"""
        return {
            "short_term": len(self.short_term_memory),
            "conversations": self.conversations.count(),
            "knowledge": self.knowledge.count()
        }
    
    def _store_to_long_term(self, memory_item: Dict):
        """Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ Ø­Ø§ÙØ¸Ù‡ Ø¨Ù„Ù†Ø¯Ù…Ø¯Øª"""
        os.makedirs("data/memory/long_term", exist_ok=True)
        
        filename = f"data/memory/long_term/{memory_item['timestamp'][:10]}.jsonl"
        with open(filename, "a", encoding="utf-8") as f:
            f.write(json.dumps(memory_item, ensure_ascii=False) + "\n")
    
    def _store_to_vector_db(self, memory_item: Dict):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± vector database"""
        try:
            self.conversations.add(
                documents=[memory_item["content"]],
                metadatas=[{
                    "role": memory_item["role"],
                    "timestamp": memory_item["timestamp"],
                    **memory_item.get("metadata", {})
                }],
                ids=[memory_item["id"]]
            )
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ vector: {e}")
    
    def _is_important_conversation(self, content: str) -> bool:
        """ØªØ´Ø®ÛŒØµ Ø§Ù‡Ù…ÛŒØª Ù…Ú©Ø§Ù„Ù…Ù‡"""
        important_keywords = [
            "ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ", "ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±", "Ø¨Ù‡ Ø®Ø§Ø·Ø± Ø¨Ø³Ù¾Ø§Ø±", "Ù…Ù‡Ù…", 
            "Ù†Ø§Ù… Ù…Ù†", "Ø§Ø³Ù… Ù…Ù†", "Ø¯ÙˆØ³Øª Ø¯Ø§Ø±Ù…", "Ø¹Ù„Ø§Ù‚Ù‡ Ø¯Ø§Ø±Ù…",
            "Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù…", "Ø´ØºÙ„", "Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡", "Ø³Ù†"
        ]
        
        content_lower = content.lower()
        return any(keyword in content_lower for keyword in important_keywords)
    
    def _generate_id(self, text: str) -> str:
        """ØªÙˆÙ„ÛŒØ¯ ID ÛŒÚ©ØªØ§"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def store_knowledge(self, topic: str, information: str, source: str = "user"):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ù†Ø´ Ø¬Ø¯ÛŒØ¯"""
        knowledge_id = self._generate_id(f"{topic}_{information}")
        
        try:
            self.knowledge.add(
                documents=[information],
                metadatas=[{
                    "topic": topic,
                    "source": source,
                    "timestamp": datetime.now().isoformat()
                }],
                ids=[knowledge_id]
            )
            print(f"ðŸ“š Ø¯Ø§Ù†Ø´ Ø¬Ø¯ÛŒØ¯ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {topic}")
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ù†Ø´: {e}")
    
    def search_knowledge(self, query: str, limit: int = 3) -> List[Dict]:
        """Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯Ø§Ù†Ø´ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡"""
        try:
            results = self.knowledge.query(
                query_texts=[query],
                n_results=limit
            )
            
            knowledge_items = []
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    metadata = results['metadatas'][0][i] if results['metadatas'] else {}
                    knowledge_items.append({
                        "information": doc,
                        "topic": metadata.get("topic", "Ù†Ø§Ù…Ø´Ø®Øµ"),
                        "source": metadata.get("source", "Ù†Ø§Ù…Ø´Ø®Øµ"),
                        "timestamp": metadata.get("timestamp", "")
                    })
            
            return knowledge_items
            
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¯Ø§Ù†Ø´: {e}")
            return []