"""
Ø³ÛŒØ³ØªÙ… Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆØ¨ Ø±ÙˆØ¨Ø§Ù‡
Ù‚Ø§Ø¨Ù„ÛŒØª Ø¬Ø³ØªØ¬Ùˆ Ùˆ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² Ø§ÛŒÙ†ØªØ±Ù†Øª
"""

import requests
import json
from typing import List, Dict, Optional
from datetime import datetime
import re
from urllib.parse import quote_plus
import time

class WebSearchEngine:
    def __init__(self):
        self.search_engines = {
            "duckduckgo": "https://api.duckduckgo.com/",
            "wikipedia": "https://fa.wikipedia.org/api/rest_v1/page/summary/",
            "google_custom": None  # Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒÙ… Ø¨Ø¹Ø¯Ø§Ù‹ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒÙ…
        }
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        print("ðŸŒ Ø³ÛŒØ³ØªÙ… Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆØ¨ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ø´Ø¯")
    
    def should_search_web(self, query: str, context: List[Dict] = None) -> bool:
        """ØªØ´Ø®ÛŒØµ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆØ¨ Ù‡Ø³Øª ÛŒØ§ Ù†Ù‡"""
        
        # Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø¯Ù‡ Ú©Ù‡ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆØ¨ Ù†Ø¯Ø§Ø±Ù†
        simple_greetings = [
            "Ø³Ù„Ø§Ù…", "Ø¯Ø±ÙˆØ¯", "ØµØ¨Ø­ Ø¨Ø®ÛŒØ±", "Ø¹ØµØ± Ø¨Ø®ÛŒØ±", "Ø´Ø¨ Ø¨Ø®ÛŒØ±",
            "Ú†Ø·ÙˆØ±ÛŒ", "Ø­Ø§Ù„Øª Ú†Ø·ÙˆØ±Ù‡", "Ø®ÙˆØ¨ÛŒ", "Ú†Ù‡ Ø®Ø¨Ø±",
            "hello", "hi", "how are you", "good morning"
        ]
        
        query_lower = query.lower().strip()
        
        # Ø§Ú¯Ø± Ù¾ÛŒØ§Ù… Ø³Ø§Ø¯Ù‡ Ø³Ù„Ø§Ù… Ùˆ Ø§Ø­ÙˆØ§Ù„â€ŒÙ¾Ø±Ø³ÛŒ Ø¨Ø§Ø´Ù‡ØŒ Ø¬Ø³ØªØ¬Ùˆ Ù†Ú©Ù†
        if any(greeting in query_lower for greeting in simple_greetings):
            return False
        
        # Ø§Ú¯Ø± Ù¾ÛŒØ§Ù… Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ Ø¨Ø§Ø´Ù‡ (Ú©Ù…ØªØ± Ø§Ø² 5 Ú©Ù„Ù…Ù‡)
        if len(query.split()) < 5:
            return False
        
        # Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ú©Ù‡ Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¬Ø¯ÛŒØ¯ Ù‡Ø³ØªÙ†Ø¯
        web_indicators = [
            "Ø¢Ø®Ø±ÛŒÙ†", "Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ†", "Ø§Ù…Ø±ÙˆØ²", "Ø§Ù„Ø§Ù†", "ÙØ¹Ù„ÛŒ", "Ø§Ø®Ø¨Ø§Ø±",
            "Ù‚ÛŒÙ…Øª", "Ù†Ø±Ø®", "Ø§Ø±Ø²", "Ø¨ÙˆØ±Ø³", "Ù‡ÙˆØ§", "Ø¢Ø¨ Ùˆ Ù‡ÙˆØ§",
            "Ú†Ù‡ Ø®Ø¨Ø±", "Ú†Ù‡ Ø§ØªÙØ§Ù‚ÛŒ", "ÙˆØ¶Ø¹ÛŒØª", "Ø¢Ù…Ø§Ø±", "ØªØ§Ø±ÛŒØ®",
            "Ú©ÛŒ", "Ú©Ø¬Ø§", "Ú†Ø·ÙˆØ±", "Ú†Ø±Ø§", "Ú†ÛŒØ³Øª", "ØªØ¹Ø±ÛŒÙ",
            "latest", "current", "today", "now", "news", "price"
        ]
        
        # Ø§Ú¯Ø± Ø´Ø§Ù…Ù„ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø§Ø´Ø¯
        if any(indicator in query_lower for indicator in web_indicators):
            return True
        
        # Ø§Ú¯Ø± Ø³Ø¤Ø§Ù„ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø¨Ø§Ø´Ø¯ Ùˆ Ø¯Ø± context Ø¬ÙˆØ§Ø¨ Ù†Ø¨Ø§Ø´Ø¯
        if "ØŸ" in query and len(query.split()) > 8 and (not context or len(context) == 0):
            return True
        
        # Ù¾ÛŒØ´â€ŒÙØ±Ø¶: Ø¬Ø³ØªØ¬Ùˆ Ù†Ú©Ù†
        return False
        specific_requests = [
            "Ø¨Ú¯Ùˆ", "ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯Ù‡", "Ø´Ø±Ø­ Ø¨Ø¯Ù‡", "Ø§Ø·Ù„Ø§Ø¹Ø§Øª", "Ø¬Ø²Ø¦ÛŒØ§Øª"
        ]
        
        if any(req in query_lower for req in specific_requests):
            return True
        
        return False
    
    async def search_and_summarize(self, query: str) -> Optional[Dict]:
        """Ø¬Ø³ØªØ¬Ùˆ Ùˆ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù†ØªØ§ÛŒØ¬"""
        
        print(f"ðŸ” Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆØ¨ Ø¨Ø±Ø§ÛŒ: {query}")
        
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø®ØªÙ„Ù
        results = []
        
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± ÙˆÛŒÚ©ÛŒâ€ŒÙ¾Ø¯ÛŒØ§ ÙØ§Ø±Ø³ÛŒ
        wiki_result = await self._search_wikipedia_fa(query)
        if wiki_result:
            results.append(wiki_result)
        
        # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± DuckDuckGo
        ddg_result = await self._search_duckduckgo(query)
        if ddg_result:
            results.extend(ddg_result)
        
        if not results:
            return None
        
        # Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù†ØªØ§ÛŒØ¬
        summary = self._summarize_results(results, query)
        
        return {
            "query": query,
            "timestamp": datetime.now().isoformat(),
            "sources": len(results),
            "summary": summary,
            "raw_results": results[:3]  # ÙÙ‚Ø· 3 Ù†ØªÛŒØ¬Ù‡ Ø§ÙˆÙ„
        }
    
    async def _search_wikipedia_fa(self, query: str) -> Optional[Dict]:
        """Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± ÙˆÛŒÚ©ÛŒâ€ŒÙ¾Ø¯ÛŒØ§ ÙØ§Ø±Ø³ÛŒ"""
        try:
            # Ø§Ø¨ØªØ¯Ø§ Ø¬Ø³ØªØ¬Ùˆ Ú©Ù†ÛŒÙ…
            search_url = "https://fa.wikipedia.org/w/api.php"
            search_params = {
                'action': 'query',
                'format': 'json',
                'list': 'search',
                'srsearch': query.replace("ØŸ", "").strip(),
                'srlimit': 1
            }
            
            search_response = requests.get(search_url, params=search_params, headers=self.headers, timeout=10)
            
            if search_response.status_code == 200:
                search_data = search_response.json()
                
                if search_data.get('query', {}).get('search'):
                    page_title = search_data['query']['search'][0]['title']
                    
                    # Ø­Ø§Ù„Ø§ Ù…Ø­ØªÙˆØ§ÛŒ ØµÙØ­Ù‡ Ø±Ø§ Ø¨Ú¯ÛŒØ±ÛŒÙ…
                    content_params = {
                        'action': 'query',
                        'format': 'json',
                        'titles': page_title,
                        'prop': 'extracts',
                        'exintro': True,
                        'explaintext': True,
                        'exsectionformat': 'plain'
                    }
                    
                    content_response = requests.get(search_url, params=content_params, headers=self.headers, timeout=10)
                    
                    if content_response.status_code == 200:
                        content_data = content_response.json()
                        pages = content_data.get('query', {}).get('pages', {})
                        
                        for page_id, page_data in pages.items():
                            if page_data.get('extract'):
                                return {
                                    "source": "ÙˆÛŒÚ©ÛŒâ€ŒÙ¾Ø¯ÛŒØ§ ÙØ§Ø±Ø³ÛŒ",
                                    "title": page_data.get('title', ''),
                                    "content": page_data.get('extract', ''),
                                    "url": f"https://fa.wikipedia.org/wiki/{page_title.replace(' ', '_')}",
                                    "type": "encyclopedia"
                                }
        
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆÛŒÚ©ÛŒâ€ŒÙ¾Ø¯ÛŒØ§: {e}")
        
        return None
    
    async def _search_duckduckgo(self, query: str) -> List[Dict]:
        """Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± DuckDuckGo"""
        try:
            # DuckDuckGo Instant Answer API
            params = {
                'q': query,
                'format': 'json',
                'no_html': '1',
                'skip_disambig': '1'
            }
            
            response = requests.get(
                "https://api.duckduckgo.com/",
                params=params,
                headers=self.headers,
                timeout=10
            )
            
            if response.status_code == 200:
                data = response.json()
                results = []
                
                # Abstract (Ø®Ù„Ø§ØµÙ‡ Ø§ØµÙ„ÛŒ)
                if data.get('Abstract'):
                    results.append({
                        "source": "DuckDuckGo",
                        "title": data.get('AbstractText', ''),
                        "content": data.get('Abstract', ''),
                        "url": data.get('AbstractURL', ''),
                        "type": "abstract"
                    })
                
                # Definition (ØªØ¹Ø±ÛŒÙ)
                if data.get('Definition'):
                    results.append({
                        "source": "ØªØ¹Ø±ÛŒÙ",
                        "title": "ØªØ¹Ø±ÛŒÙ",
                        "content": data.get('Definition', ''),
                        "url": data.get('DefinitionURL', ''),
                        "type": "definition"
                    })
                
                # Answer (Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ…)
                if data.get('Answer'):
                    results.append({
                        "source": "Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ…",
                        "title": "Ù¾Ø§Ø³Ø®",
                        "content": data.get('Answer', ''),
                        "url": "",
                        "type": "direct_answer"
                    })
                
                # Ø§Ú¯Ø± Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ Ù†ÛŒØ§ÙØªÛŒÙ…ØŒ ÛŒÚ© Ù¾Ø§Ø³Ø® Ø¹Ù…ÙˆÙ…ÛŒ Ø¨Ø¯Ù‡ÛŒÙ…
                if not results:
                    results.append({
                        "source": "Ø³ÛŒØ³ØªÙ…",
                        "title": "Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ",
                        "content": f"Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ù‚ÛŒÙ‚ÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡ '{query}' Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³ØªØŒ Ø§Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ù†Ø´ Ø¹Ù…ÙˆÙ…ÛŒâ€ŒØ§Ù… Ú©Ù…Ú©ØªØ§Ù† Ú©Ù†Ù….",
                        "url": "",
                        "type": "fallback"
                    })
                
                return results
        
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ DuckDuckGo: {e}")
        
        return []
    
    def _summarize_results(self, results: List[Dict], original_query: str) -> str:
        """Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù†ØªØ§ÛŒØ¬ Ø¬Ø³ØªØ¬Ùˆ"""
        
        if not results:
            return "Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø±ØªØ¨Ø·ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ú©Ø±Ø¯Ù…."
        
        summary_parts = []
        
        for result in results[:3]:  # ÙÙ‚Ø· 3 Ù†ØªÛŒØ¬Ù‡ Ø§ÙˆÙ„
            content = result.get('content', '').strip()
            source = result.get('source', 'Ù…Ù†Ø¨Ø¹ Ù†Ø§Ù…Ø´Ø®Øµ')
            
            if content:
                # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„ Ù…Ø­ØªÙˆØ§
                if len(content) > 200:
                    content = content[:200] + "..."
                
                summary_parts.append(f"ðŸ“Œ {source}: {content}")
        
        if summary_parts:
            summary = "\n\n".join(summary_parts)
            summary += f"\n\nðŸ” Ø§ÛŒÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø§Ø² Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§ÛŒÙ†ØªØ±Ù†Øª Ø¨Ø±Ø§ÛŒ Ø³Ø¤Ø§Ù„ Ø´Ù…Ø§ Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯."
            return summary
        
        return "Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ù¾ÛŒØ¯Ø§ Ø´Ø¯ Ø§Ù…Ø§ Ù‚Ø§Ø¨Ù„ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ø¨ÙˆØ¯."
    
    def is_online(self) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø§ÛŒÙ†ØªØ±Ù†Øª"""
        try:
            response = requests.get("https://www.google.com", timeout=5)
            return response.status_code == 200
        except:
            return False