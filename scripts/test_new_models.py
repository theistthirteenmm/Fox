#!/usr/bin/env python3
"""
ğŸ§ª ØªØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø±ÙˆØ¨Ø§Ù‡ 2025
Ø¢Ø²Ù…Ø§ÛŒØ´ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡
"""

import asyncio
import requests
import time
from datetime import datetime
import json

class ModelTester:
    def __init__(self):
        self.ollama_url = "http://localhost:11434"
        self.models = {
            "persian": "partai/dorna-llama3:8b-instruct-q8_0",
            "general": "llama3.3:70b",
            "reasoning": "deepseek-r1:7b",
            "code": "deepseek-coder-v2:16b",
            "fast": "llama3.2:3b",
            "multilingual": "qwen2.5:32b"
        }
        
        self.test_messages = {
            "persian": "Ø³Ù„Ø§Ù…! Ú†Ø·ÙˆØ±ÛŒØŸ Ø§Ù…Ø±ÙˆØ² Ú†Ù‡ Ú©Ø§Ø±Ù‡Ø§ÛŒÛŒ Ø§Ù†Ø¬Ø§Ù… Ø¯Ø§Ø¯ÛŒØŸ",
            "code": "ÛŒÚ© function Ø¯Ø± Python Ø¨Ù†ÙˆÛŒØ³ Ú©Ù‡ Ø§Ø¹Ø¯Ø§Ø¯ ÙÛŒØ¨ÙˆÙ†Ø§Ú†ÛŒ Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ù†Ø¯",
            "reasoning": "Ú†Ø±Ø§ Ø¢Ø³Ù…Ø§Ù† Ø¢Ø¨ÛŒ Ø§Ø³ØªØŸ Ø¯Ù„ÛŒÙ„ Ø¹Ù„Ù…ÛŒ Ø§ÛŒÙ† Ù¾Ø¯ÛŒØ¯Ù‡ Ø±Ø§ ØªØ­Ù„ÛŒÙ„ Ú©Ù†",
            "multilingual": "Please translate this to Persian: Hello, how are you today?",
            "fast": "Ú†Ù†Ø¯ ØªØ§ØŸ",
            "general": "Ø¯Ø± Ù…ÙˆØ±Ø¯ ØªØ£Ø«ÛŒØ± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¨Ø± Ø¢ÛŒÙ†Ø¯Ù‡ Ø¨Ø´Ø±ÛŒØª ÛŒÚ© ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ Ø§Ø±Ø§Ø¦Ù‡ Ø¨Ø¯Ù‡"
        }
    
    def check_model_availability(self, model_name: str) -> bool:
        """Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø¨ÙˆØ¯Ù† Ù…Ø¯Ù„"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                available_models = [m["name"] for m in models]
                return model_name in available_models
            return False
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø¯Ù„ {model_name}: {e}")
            return False
    
    def test_model_response(self, model_name: str, message: str) -> dict:
        """ØªØ³Øª Ù¾Ø§Ø³Ø® Ù…Ø¯Ù„"""
        start_time = time.time()
        
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": model_name,
                    "prompt": message,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "max_tokens": 200
                    }
                },
                timeout=60
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            if response.status_code == 200:
                result = response.json()
                return {
                    "success": True,
                    "response": result.get("response", ""),
                    "response_time": response_time,
                    "model": model_name,
                    "tokens": len(result.get("response", "").split())
                }
            else:
                return {
                    "success": False,
                    "error": f"HTTP {response.status_code}: {response.text}",
                    "response_time": response_time,
                    "model": model_name
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "response_time": time.time() - start_time,
                "model": model_name
            }
    
    def run_comprehensive_test(self):
        """Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øª Ø¬Ø§Ù…Ø¹"""
        print("ğŸ§ª Ø´Ø±ÙˆØ¹ ØªØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø±ÙˆØ¨Ø§Ù‡ 2025")
        print("=" * 60)
        
        results = {}
        
        # Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø¨ÙˆØ¯Ù† Ù…Ø¯Ù„â€ŒÙ‡Ø§
        print("\nğŸ“‹ Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø¨ÙˆØ¯Ù† Ù…Ø¯Ù„â€ŒÙ‡Ø§:")
        available_models = {}
        for category, model_name in self.models.items():
            is_available = self.check_model_availability(model_name)
            available_models[category] = is_available
            status = "âœ… Ù…ÙˆØ¬ÙˆØ¯" if is_available else "âŒ ØºÛŒØ±Ù…ÙˆØ¬ÙˆØ¯"
            print(f"  {category}: {model_name} - {status}")
        
        print("\n" + "=" * 60)
        
        # ØªØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
        for category, model_name in self.models.items():
            if not available_models[category]:
                print(f"\nâ­ï¸  Ø±Ø¯ Ø´Ø¯Ù† ØªØ³Øª {category} (Ù…Ø¯Ù„ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª)")
                continue
                
            print(f"\nğŸ” ØªØ³Øª Ù…Ø¯Ù„ {category}: {model_name}")
            print("-" * 40)
            
            test_message = self.test_messages.get(category, self.test_messages["persian"])
            print(f"ğŸ“ Ù¾ÛŒØ§Ù… ØªØ³Øª: {test_message}")
            
            result = self.test_model_response(model_name, test_message)
            results[category] = result
            
            if result["success"]:
                print(f"âœ… Ù…ÙˆÙÙ‚ - Ø²Ù…Ø§Ù† Ù¾Ø§Ø³Ø®: {result['response_time']:.2f}s")
                print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª: {result['tokens']}")
                print(f"ğŸ’¬ Ù¾Ø§Ø³Ø®: {result['response'][:100]}...")
            else:
                print(f"âŒ Ù†Ø§Ù…ÙˆÙÙ‚ - Ø®Ø·Ø§: {result['error']}")
        
        # Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬
        print("\n" + "=" * 60)
        print("ğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬:")
        print("=" * 60)
        
        successful_tests = [k for k, v in results.items() if v.get("success", False)]
        failed_tests = [k for k, v in results.items() if not v.get("success", False)]
        
        print(f"âœ… ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆÙÙ‚: {len(successful_tests)}")
        print(f"âŒ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù…ÙˆÙÙ‚: {len(failed_tests)}")
        
        if successful_tests:
            print(f"\nğŸ‰ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¢Ù…Ø¯: {', '.join(successful_tests)}")
            
            # Ø¨Ù‡ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯
            fastest_model = min(
                [results[k] for k in successful_tests], 
                key=lambda x: x["response_time"]
            )
            print(f"âš¡ Ø³Ø±ÛŒØ¹â€ŒØªØ±ÛŒÙ† Ù…Ø¯Ù„: {fastest_model['model']} ({fastest_model['response_time']:.2f}s)")
        
        if failed_tests:
            print(f"\nâš ï¸  Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù†ÛŒØ§Ø²Ù…Ù†Ø¯ Ø¨Ø±Ø±Ø³ÛŒ: {', '.join(failed_tests)}")
        
        # ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§
        print("\nğŸ’¡ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§:")
        if len(successful_tests) >= 3:
            print("âœ… Ø³ÛŒØ³ØªÙ… Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø³Øª")
        elif len(successful_tests) >= 1:
            print("âš ï¸  Ø¨Ø±Ø®ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¯Ø§Ø±Ù†Ø¯")
            print("ğŸ”§ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯: scripts\\download_models.bat")
        else:
            print("âŒ Ù‡ÛŒÚ† Ù…Ø¯Ù„ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª")
            print("ğŸ”§ Ø§Ø¨ØªØ¯Ø§ Ollama Ø±Ø§ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯")
        
        return results

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    tester = ModelTester()
    
    print(f"ğŸ¦Š ØªØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ¨Ø§Ù‡ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        results = tester.run_comprehensive_test()
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
        with open("test_results.json", "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± test_results.json Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ØªØ³Øª Ù…ØªÙˆÙ‚Ù Ø´Ø¯")
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡: {e}")

if __name__ == "__main__":
    main()